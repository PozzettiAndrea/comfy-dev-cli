# <span id="page-0-1"></span>ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion

Remy Sabathier<sup>1</sup>,<sup>3</sup> David Novotny<sup>2</sup> Niloy J. Mitra<sup>3</sup> Tom Monnier<sup>1</sup> <sup>1</sup>Meta Reality Labs <sup>2</sup>SpAItial <sup>3</sup>University College London

<https://remysabathier.github.io/actionmesh/>

![](_page_0_Figure_3.jpeg)

<span id="page-0-0"></span>Figure 1. ActionMesh. Our model generates 3D meshes 'in action' from a wide range of inputs such as a text prompt, a video, an image + an animation text prompt, or a 3D mesh + an animation text prompt. Unlike previous approaches, our method is not only fast, but also rig-free and topology consistent. These properties are convenient in practice, *e.g*., they allow the seamless animation of complex 3D shapes like an octopus with maracas (top left) or the automatic transfer of the mesh texture throughout the animation (bottom right).

## Abstract

*Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes 'in action' in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed 'temporal 3D diffusion'. Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid itera-*

*tion and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.*

## 1. Introduction

The ability to automatically produce animated 3D objects from simple user inputs is a core computer vision problem that holds high promise for any 3D-content application, like video games, animated movies and commercials, or augmented/virtual reality. However, despite recent progress, most works share three main limitations. First, they are specific to limited setups with a predefined input modality (*e.g*., a video) and predefined object categories (*e.g*., bipeds or rig-able objects). Second, they often rely on long (30-45 minutes) optimization loops, which are slow and prone to local minima. Third, the overall output quality does not meet the production criteria.

In this paper, we introduce *ActionMesh*, a feed-forward generative model that is simple, scalable, and computes <span id="page-1-0"></span>production-ready 3D meshes 'in action' from diverse inputs. At its core, ActionMesh is a new video-to-4D model that predicts an animated 3D mesh given a video as input. Drawing inspiration from early video models, we extend existing 3D diffusion models with a temporal axis and separate the 3D generation from the animation prediction. Our model runs in two stages. First, we derive a *temporal 3D diffusion model* from a pretrained 3D latent diffusion model, which produces a sequence of synchronized latents representing time-varying but independent 3D meshes. Second, we design a *temporal 3D autoencoder*, which converts a generic sequence of 3D meshes into the deformations of a chosen reference mesh, yielding an animation with constant topology. Importantly, for both stages, we build upon models with strong 3D priors to balance for the lack of 3D animated data.

Compared to existing approaches, ActionMesh is fast (3 minutes for 16-frame video) and produces meshes that are rig-free and topology consistent. These properties are convenient in practice since (i) it allows the animation of complex 3D shapes where rigging is unknown or difficult, and (ii) it automatically preserves the mesh attributes, like a texture, throughout the animation. In addition, thanks to its design, we show that our model can be extended to other generative tasks like text-to-4D, {image+text}-to-4D or {3D+text} to-animation, as well as related applications like retargeting/motion transfer, as illustrated in Figure [1](#page-0-0) and Figure [4.](#page-5-0) We evaluate our video-to-4D model qualitatively on the Consistent4D benchmark [\[14\]](#page-8-0) and quantitatively on a newly introduced Objaverse [\[8\]](#page-8-1) benchmark. Comparing against state-of-the-art methods, including DreamMesh4D [\[18\]](#page-8-2), LIM [\[31\]](#page-9-0), V2M4 [\[4\]](#page-8-3), and the concurrent work Shape-Gen4D [\[50\]](#page-9-1), we show that ActionMesh consistently outperforms competitors on both geometric accuracy and correspondence quality while achieving a speed-up of roughly 10×. Code and pretrained weights are available on our webpage: <https://remysabathier.github.io/actionmesh/>.

Summary. Our main contributions are three-fold:

- A fast feed-forward model called ActionMesh that generates animated 3D meshes from diverse inputs with an unprecedented speed and quality.
- A temporal 3D diffusion model that produces synchronized shape latents by minimally extending pretrained 3D diffusion backbones.
- A temporal 3D autoencoder that assembles independent shapes into a single topology-consistent animation via the deformation of a reference mesh.

## 2. Related Work

3D foundational models. The seminal work of Zhang *et al*. [\[54\]](#page-9-2) introduces 3DShape2VecSet, a neural field supported by a set of latent vectors (a vecset), designed for

scalable 3D encoding of meshes and point clouds, and explicitly targeting generative diffusion. Trellis [\[45\]](#page-9-3) generalizes this idea to structured 3D latents, scaling to large models that can decode into multiple 3D representations such as radiance fields, Gaussian splats, and meshes. Building on vecset-style latents, Craftsman [\[16\]](#page-8-4) performs 3Dnative diffusion over latent sets coupled with a geometry refiner, enabling high-quality mesh synthesis and editing; Dora-VAE [\[5\]](#page-8-5) improves VAE sampling on similar representations with a strong emphasis on sharp edges and highfrequency details. CLAY [\[56\]](#page-9-4) further advances controllable 3D generation with a large 3D-native latent DiT for geometry and a dedicated diffusion model for materials. In parallel, LRM [\[12\]](#page-8-6) and follow-up works like [\[34,](#page-9-5) [38,](#page-9-6) [52\]](#page-9-7) demonstrate that large transformers trained on massive image collections can reconstruct high-fidelity 3D NeRFs or Gaussians from a single view. Among recent image-to-3D mesh generators, Hunyuan3D [\[39\]](#page-9-8) predicts geometry with a flow-based DiT and texture with a separate model, generating highresolution assets from text or images; TripoSG [\[17\]](#page-8-7) uses a large rectified-flow transformer for high-fidelity mesh reconstruction. While image-to-3D models provide powerful frame-level priors, they process each instance independently and do not explicitly model temporal correspondences, making it difficult to produce an animated mesh with a constant topology across frames.

Video-to-4D via optimization. Targeting dynamic 4D content, a common strategy first synthesizes multi-view or multi-frame videos from a monocular sequence and then optimizes a 4D representation. SV4D and SV4D 2.0 [\[46,](#page-9-9) [48\]](#page-9-10) unify multi-frame, multi-view video diffusion to supervise dynamic NeRFs, significantly improving spatio-temporal consistency but still requiring per-scene optimization. CAT4D [\[43\]](#page-9-11) similarly converts a monocular video into multi-view sequences and then optimizes deformable 3D Gaussians, offering strong reconstructions and controllable camera and time, but without vertex-tovertex correspondences on a single topology. Related approaches [\[14,](#page-8-0) [28,](#page-8-8) [41,](#page-9-12) [44,](#page-9-13) [51,](#page-9-14) [53,](#page-9-15) [55\]](#page-9-16) also rely on diffusion or generative priors to supervise dynamic Gaussians or NeRFs, typically with temporal regularizers or sparse controls and a subsequent optimization or training stage for each scene. DreamMesh4D [\[18\]](#page-8-2) predicts meshes via a mesh–Gaussian hybrid with sparsely controlled deformation followed by optimization. V2M4 [\[4\]](#page-8-3) enforces topology and texture consistency through a multi-stage pipeline that includes camera search, reposing, pairwise registration, and global texture optimization. LIM [\[31\]](#page-9-0) learns to interpolate a 3D implicit field over time and extracts a UV-textured mesh sequence with consistent topology through a test-time optimization. These methods achieve high-quality 4D reconstructions but rely on post-optimization or per-scene training.

<span id="page-2-1"></span><span id="page-2-0"></span>![](_page_2_Figure_0.jpeg)

Figure 2. Overview. Given an input video, our model generates an animated 3D mesh in two stages. (Stage I) After computing a reference mesh latent z ∗ <sup>1</sup> with an off-the-shelf image-to-3D model, we use our temporal 3D diffusion model to produce from z ∗ <sup>1</sup> and the video, a sequence of time-varying but independent 3D meshes. (Stage II) Our temporal 3D autoencoder takes theses shapes as input and predicts, for each shape in the sequence, a deformation field of the reference mesh vertices, thus yielding an animated 3D mesh with consistent topology.

Feed-forward 4D reconstruction. A complementary line of work reconstructs or generates 4D content in a single forward pass, avoiding test-time optimization. Motion2VecSets [\[2\]](#page-8-9) proposes a 4D diffusion model that denoises compressed latent sets for dynamic surface reconstruction from point-cloud sequences. L4GM [\[29\]](#page-8-10) predicts a sequence of 3D Gaussian splats from a monocular video in one pass and then upsamples them temporally via a learned interpolation model, but still operates entirely in Gaussian space rather than producing an animated mesh with fixed topology. Similarly, 4DGT [\[47\]](#page-9-17) learns a transformer that directly predicts 4D Gaussians from real monocular videos, again operating in Gaussian space instead of explicit meshes. These existing feed-forward methods either produce Gaussians or neural fields instead of a mesh with constant topology. Concurrently, ShapeGen4D [\[50\]](#page-9-1) extends a pre-trained 3D generative model by introducing temporal attention, enabling feed-forward prediction of video-conditioned 4D meshes. Unlike our approach, it does not explicitly enforce a single, globally consistent mesh topology across the sequence.

Animation-ready 4D assets. Several works focus on making existing 3D assets *animation-ready* by predicting rigs, skinning weights, or deformation fields, instead of reconstructing geometry from raw videos. Make-It-Animatable [\[10\]](#page-8-11) predicts skeletons, skinning weights, and pose rectification for meshes. MagicArticulate [\[36\]](#page-9-18) similarly generates articulation-ready rigs and deformations for a wide range of shapes. RigAnything [\[21\]](#page-8-12) extends auto-rigging to

diverse object categories via an autoregressive transformer that predicts hierarchical skeletons and skinning weights. For Gaussian-based assets, RigGS [\[49\]](#page-9-19) recovers skeletons and skinning directly from videos, enabling articulated motion in the space of 3D Gaussians. DriveAnyMesh [\[33\]](#page-9-20) deforms an existing mesh given a monocular video, using latent 4D diffusion over point-trajectory sequences. These methods operate on meshes or 3D Gaussians provided a priori, and focus on rigging and animation, rather than reconstructing an animated mesh from a single input video.

# 3. Method

Our goal is to generate animated 3D meshes from various user inputs. Instead of building input-specific models, we cast multiple generative tasks into the core *video-to-4D* problem, whose goal is to generate an animated 3D mesh given a single video. To address this, we harness pretrained 3D generators to balance for the lack of 3D animated data. Specifically, we build upon the 3D latent diffusion framework of 3DShape2VecSet [\[54\]](#page-9-2) (Section [3.1\)](#page-3-0) and propose minimal modifications leading to our two-stage model called *Action-Mesh*. First, we introduce a temporal 3D diffusion model, predicting a sequence of time-varying but independent 3D meshes (Section [3.2\)](#page-3-1). Second, we present a temporal 3D autoencoder, converting a sequence of independent 3D shapes into an animated mesh with a constant topology (Section [3.3\)](#page-4-0). Finally, we discuss how to apply our model to other scenarios in Section [3.4.](#page-5-1) Figure [2](#page-2-0) shows an overview of ActionMesh. <span id="page-3-3"></span>**Terminology and problem setting.** We use '3D mesh' to describe a triangular 3D mesh denoted by  $\mathcal{M}=(\mathbf{V},\mathbf{F})$ , where  $\mathbf{V}\in\mathbb{R}^{N_{\mathrm{v}}\times3}$  are the vertex positions and  $\mathbf{F}\in\{1,...,N_{\mathrm{V}}\}^{N_{\mathrm{f}}\times3}$  captures the face connectivity. We use '4D mesh' to describe a sequence of time-varying but *independent* 3D meshes that do not share the same topology, which we denote by  $\{(\mathbf{V}_k,\mathbf{F}_k)\}_{k=1}^N$ . Finally, we call 'animated 3D mesh' a sequence of 3D meshes sharing the *same* topology and denoted by  $\{(\mathbf{V}_k,\mathbf{F})\}_{k=1}^N$ . Note that an animated 3D mesh is a particular form of a 4D mesh.

Let  $\{\mathbf{I}_k\}_{k=1}^N$  be a video, where  $\mathbf{I}_k \in \mathbb{R}^{H \times W \times 3}$  is an RGB frame depicting an object in motion at framestep  $t_k$ . Our objective is to predict an animated 3D mesh corresponding to the input video. Specifically, we aim at predicting a reference mesh  $\mathcal{M} = (\mathbf{V}, \mathbf{F})$  as well as its vertex position updates  $\mathbf{V}_k$  for each framestep  $t_k$ , such that  $\mathbf{V}_k$  represents the new vertices of  $\mathcal{M}$  matching the motion depicted by  $\mathbf{I}_k$ .

#### <span id="page-3-0"></span>3.1. Background: 3DShape2VecSet

Following the paradigm of latent diffusion models [30], 3DShape2VecSet learns a 3D diffusion model comprising two stages: (i) a variational autoencoder (VAE) made of an encoder  $\mathcal{E}_{3D}$  and a decoder  $\mathcal{D}_{3D}$  to encode 3D shapes into a compact latent space, and (ii) a diffusion model  $\mathcal{G}_{3D}$ predicting latents conditioned on an input image. Specifically, given a dense set of points P sampled on the surface, a sparse set of query vectors Q (called vector set or *VecSet*, which is either learned or subsampled from P) is encoded by a cross-attention layer using P as context. Then, the query vectors are passed to several self-attention layers to produce the low-dimensional latent z. The decoder  $\mathcal{D}_{3D}$  takes the latent z, processes it with several self-attention layers and uses the output as context to a cross-attention layer producing the occupancy (or signed distance) of a query 3D point. Finally, a mesh is computed by querying the decoder on a dense grid of points and running a meshification algorithm like marching cubes [23]. Note that this architecture is reminiscent of Perceiver IO [13] used in NLP.

The generative model  $\mathcal{G}_{3D}$  is a diffusion transformer [26], which takes the form of a decoder-only transformer [40] where each block has an extra cross-attention layer to incorporate the conditioning signal. In follow-up works, the conditioning signal is typically an image that is described using a frozen DINOv2 [25]. Many advanced image-to-3D models build on top of VecSet, *e.g.*, CLAY [56], Craftsman [16], TripoSG [17] or Hunyuan3D [39]. In this work, we adopt TripoSG as our backbone for its state-of-the-art performance and its open-source implementation, but our formulation should be applicable to other VecSet backbones. One specificity of TripoSG is that it relies on rectified flow [20, 22] and that the flow timestep  $s \in [0, 1000]$  is Fourier-embedded and concatenated as an additional token (see [17]).

<span id="page-3-2"></span>![](_page_3_Picture_5.jpeg)

Figure 3. **Image-to-3D results on video frames.** Running an image-to-3D model on each frame produces meshes exhibiting inconsistent global orientation (left) or inconsistent geometric details (right), even when using identical Gaussian noise in the denoiser.

#### <span id="page-3-1"></span>3.2. Stage I: Temporal 3D Diffusion

Our goal in the first stage is to produce a 4D mesh, *i.e.*, a sequence of meshes without consistent topology, from a monocular video. A naive approach is to apply an off-the-shelf image-to-3D generator independently on all frames of the video. However, we found that this per-frame generation exhibits severe inconsistencies across frames, such as inconsistent 3D orientations or geometric errors that manifest as a surface flickering through time (see Figure 3). This is somehow expected since this naive approach lacks a mechanism enforcing consistency across frames.

To encourage cross-frame consistency, we introduce *temporal 3D diffusion models*. Drawing inspiration from multi-image models derived from pretrained image models [9, 11, 32, 35], we propose to augment pretrained 3D diffusion models with a temporal axis to encourage the generations to be synchronized. Specifically, we introduce two minimal changes to the original architecture, namely *inflated attention* and *masked generation*, which we describe next. Note that we use 'temporal 3D diffusion' instead of '4D diffusion' since it is more accurate and it differentiates from recent 4D diffusion works extending image/video models to handle multi-views [19, 42, 55]. Figure 2 illustrates our resulting diffusion model architecture.

Inflated attention. Given a sequence of N frames, we build a model that outputs corresponding 3D latents in a synchronized fashion. Inspired by MVDream [32] for multiview generation, we propose to *inflate* the existing self-attention layers to allow the cross-frame synchronization of latents. There are two benefits: (i) the tokens now attend to all tokens across frames, and (ii) we leverage existing layers that are already pretrained. Specifically, let  $\mathbf{X} \in \mathbb{R}^{N \times T \times D}$  be T tokens of dimension D corresponding to the N frames. Inflated self-attention (infattn) is applied by reshaping the tensor, applying standard self-attention (selfattn) and re-

<span id="page-4-1"></span>shaping back for subsequent layers as:

$$infattn(\mathbf{X}) = reshape^{-1}(selfattn(reshape(\mathbf{X}))),$$
 (1)

where reshape flattens the first two dimensions such that  $\operatorname{reshape}(\mathbf{X}) \in \mathbb{R}^{1 \times NT \times D}$  and  $\operatorname{reshape}^{-1}$  is the reverse operation. To reduce the  $(NT)^2$  complexity overhead, we use the efficient FlashAttention2 implementation [6]. This mechanism improved the consistency across latents, yet, we still observed small jittering across consecutive frames. To mitigate this issue, we propose to inject the relative frame position information via rotary positional embedding [37] inside the inflated attention layers. We found this simple solution to generate smoother motions across frames. Note that these simple modifications not only allow us to finetune from pretrained 3D diffusion models, but also to directly reuse the pretrained 3D autoencoders.

**Masked generation.** The model described above generates consistent 4D output from a given video, but it does not allow us to easily start the generation from known 3D meshes, which has practical applications (see Sec. 3.4). Hence, we turn our model into a *masked generative model* [3, 15] where some 3D latents in the sequence are known and only the remaining 'masked' latents need to be generated. Note that such masked adaptation of pretrained generative models is reminiscent of multi-view models like CAT3D [9].

To do so with minimal architectural changes, we propose to maintain some noise-free 3D latents during our temporal 3D diffusion model training, similar to CAT3D [9]. Specifically, let  $N_S$  be the number of source latents and  $N_T$ be the number of target latents such that  $N = N_{\rm S} + N_{\rm T}$ . During training, given an input sequence of 3D latents  $\mathbf{Z} = \{\mathbf{z}_k\}_{k=1}^N \in \mathbb{R}^{N \times T \times D}$ , we randomly sample  $N_{\mathrm{S}}$  latents and keep the latter noise-free before feeding the sequence to the denoiser. To inform the model about the noise-free latents, we set the flow matching step to 0, which is a more natural solution than CAT3D's binary mask injection. Furthermore, we do not apply the diffusion loss on these source latents during training. During inference, given a set of Jsource meshes  $\{\mathcal{M}_{k_i}\}_{i=1}^J$ , we first use the 3D encoder  $\mathcal{E}_{3D}$ to compute for each  $k \in \{k_j\}_{j=1}^J$  the corresponding 3D latent  $\mathbf{z}_k^* = \mathcal{E}_{3D}(\mathcal{M}_k)$ . Then, before each denoising step, we copy the clean latents into the noisy latent sequence, allowing all noised tokens to attend to the latent representations of the known meshes. Technically, our model now corresponds to a masked temporal 3D diffusion model.

To run inference from a single video, we first use an off-the-shelf image-to-3D generator, which can be applied to any frame  $I_k$  of the video. In particular, this allows the selection of a frame where the object is well visible and free of distortion and motion-blur. After recovering a 3D mesh, we can then apply our masked model described above. Note that this process is agnostic to the image-to-3D model

used, therefore it would directly benefit from advances in this area. Unless stated otherwise, we use TripoSG [17] for the image-to-3D model. See the supplemental for details.

#### <span id="page-4-0"></span>3.3. Stage II: Temporal 3D Autoencoder

Using our temporal 3D diffusion described above, we can predict a generic 4D mesh  $\{(\mathbf{V}_k, \mathbf{F}_k)\}_{k=1}^N$  corresponding to the input video. However, this 4D representation is impractical because the mesh topology changes throughout the sequence, preventing downstream applications such as texturing. We thus aim at computing a 4D output corresponding to the explicit animation of a reference mesh (V, F). For this purpose, we propose to predict time-dependent vertex deformations  $\delta_k$  such that  $(\mathbf{V} + \delta_k, \mathbf{F})$  approximates the surface of  $(\mathbf{V}_k, \mathbf{F}_k)$ . While prior works rely on slow optimization algorithms to solve this task [4, 18, 31], we instead solve it with a feed-forward autoencoder taking the time-varying 3D meshes as input and outputting temporal deformation fields applied to the reference mesh. We do so by starting from a pretrained VecSet-based VAE which we modify to handle temporal 3D data and produce deformation field outputs, hence yielding a model we term temporal 3D autoencoder. Similar to the original VecSet VAE, such an autoencoder is also translational, as it translates a sequence of point clouds into a sequence of deformation fields.

**Formulation.** Given a sequence of N independent meshes, we first sample 3D point clouds for each mesh and pass them independently through the frozen 3D encoder  $\mathcal{E}_{3D}$  to obtain shape latents  $\mathbf{Z} = \{\mathbf{z}_k\}_{k=1}^N$ . Importantly, this part is identical to the original 3D autoencoder, which is critical for keeping consistency between the latents predicted by our temporal 3D diffusion and the latents of this temporal 3D autoencoder. On the other hand, our decoder  $\mathcal{D}_{4D}$  differs from the original  $\mathcal{D}_{3D}$  by ingesting the entire sequence of latents Z and predicting for each latent the corresponding 3D deformation field of the reference mesh. Concretely, given two arbitrary framesteps  $t_i$  and  $t_i$ , the decoder first processes all tokens with self-attention layers and then outputs the displacement from  $t_i$  to  $t_j$  of a query 3D point via a final cross-attention layer. To indicate source and target, the framesteps  $(t_i, t_j)$  are Fourier-embedded, concatenated, and injected as an extra token. During training, the query points correspond to 3D points randomly sampled on the source mesh surface whereas during inference, we feed the 3D vertex positions of the reference mesh. In practice, we augment such query points with their normals and found that such local geometry helps disambiguate points that are spatially close yet topologically distant. Following insights from our temporal 3D diffusion model, we encourage cross-shape consistency by inflating the decoder's self-attention layers and encoding relative position offsets with rotary embeddings. The autoencoder is illustrated in Figure 2.

#### <span id="page-5-4"></span><span id="page-5-1"></span>3.4. Applications

As described above, ActionMesh predicts an animated 3D mesh given a video, thus solving a **video-to-4D** problem. One of its specificities is its masked generative modeling which enables us to incorporate known 3D shapes in the generation process. This characteristic not only allows us to solve a **{3D+video}-to-animation** problem, but also to unlock several useful applications that we describe next. Figures 1 and 4 showcase some of these applications.

**{3D+text}-to-animation.** To predict an animation given a mesh and a text prompt describing the motion, we first render the mesh using a frontal viewpoint and a white background to get an image  $\mathbf{I}_1$ . Then, we use an off-the-shelf video model to animate the image  $\mathbf{I}_1$  given the text description, yielding a video  $\{\mathbf{I}_k\}_{k=1}^N$ . Finally, we apply ActionMesh with the known 3D and the generated video.

{Image+text}-to-4D. To predict an animated 3D mesh from an image depicting an object and a text prompt describing the motion, we use an off-the-shelf image-to-3D model to recover a 3D mesh, and then use our {3D+text}-to-animation process.

**Text-to-4D.** To predict an animated 3D mesh from a single text prompt, there are two possibilities: (i) we can run a video model to generate a video from the text and use ActionMesh; or (ii) we can use an image generator to compute an image, and then run our {image+text}-to-animation process. In practice, we use the latter option.

**Motion transfer / retargeting.** Although our model was not explicitly trained for retargeting, we found that it can transfer motion from an input video representing an object A to a different 3D object B. Specifically, this is done by simply running our {3D+video}-to-animation process with inconsistent objects.

Animation extrapolation. Given its autoregressive modeling, ActionMesh can also extrapolate animations, for instance generating coherent animations from long video sequences. Concretely, we split the long video into chunks and use the first chunk as input to our video-to-4D component. Then, we iterate the {3D+video}-to-animation process on the ensuing chunks, by recursively inputting the 3D output corresponding to the last frame of the previous chunk.

#### 4. Experiments

We evaluate our model on the common video-to-4D task. We first compare to the state of the art through a quantitative comparison on an in-house benchmark constructed from Objaverse [8] as well as a qualitative analysis on the standard Consistent4D benchmark [14] (Section 4.1). Then, we present additional results corresponding to other applications of our method and we conduct an ablation study of our key components (Section 4.2).

<span id="page-5-3"></span>Table 1. **Quantitative results on Objaverse.** We report results from state-of-the-art prior works, namely LIM [31], DreamMesh4D [18] and V2M4 [4]. Our method outperforms competitors across all metrics, while being significantly faster.

| Method    | Time  | CD-3D ↓ | CD-4D↓ | CD-M↓ |
|-----------|-------|---------|--------|-------|
| LIM [31]  | 15min | 0.095   | 0.127  | 0.258 |
| DM4D [18] | 35min | 0.095   | 0.140  | 0.247 |
| V2M4 [4]  | 35min | 0.063   | 0.223  | 0.500 |
| Ours      | 3min  | 0.050   | 0.069  | 0.137 |

<span id="page-5-0"></span>![](_page_5_Picture_11.jpeg)

Figure 4. **Motion transfer results**. Our model is able to accurately transfer the motion from a source video to target meshes, even if the objects are inconsistent.

#### <span id="page-5-2"></span>4.1. Comparison to SOTA

**Baselines.** We compare our model to four state-of-the-art video-to-4D methods, namely LIM [31], DreamMesh4D (DM4D) [18], V2M4 [4] and ShapeGen4D (SG4D) [50]. We use the official open-source implementations for DreamMesh4D and V2M4. For LIM, the official model is not released, so we trained a re-implementation based on the same dataset as ours. For ShapeGen4D, the code is not publicly available, so we only compare on the qualitative examples from Consistent4D evaluation set. Note that ShapeGen4D work is concurrent to ours but we include their results for completeness.

<span id="page-6-2"></span><span id="page-6-1"></span>![](_page_6_Figure_0.jpeg)

Figure 5. Qualitative comparison on Consistent4D [\[14\]](#page-8-0). LIM [\[31\]](#page-9-0) and DM4D [\[18\]](#page-8-2) tend to produce coarse geometries that lack details. V2M4 [\[4\]](#page-8-3) and SG4D [\[50\]](#page-9-1) recover sharper details but leave artifacts and partial drift. In contrast, our model preserves the highest geometric fidelity across frames with a strong temporal consistency. See our supplementary for additional examples.

Quantitative comparison on Objaverse. Since there is no open-source quantitative benchmark, we build our own using Objaverse [\[7,](#page-8-26) [8\]](#page-8-1). Specifically, we evaluate on 32 animated scenes and report three metrics that are complementary. First, we evaluate the *per-frame* 3D reconstruction quality by aligning, for each frame, the predicted mesh with ICP [\[1\]](#page-8-27) and computing the chamfer distance between ground-truth and prediction (CD-3D). Second, the 4D reconstruction quality is evaluated by aligning the predicted mesh sequence with a global ICP applied on the first mesh, and averaging the chamfer distance (CD-4D). Third, we evaluate motion fidelity with a chamfer-like distance tailored to quantify motion (CD-M). Specifically, after aligning the mesh sequence with a global ICP, we establish nearest neighbor correspondences using the first mesh. Then, for each remaining frame, we evaluate the bidirectional distance between corresponding points. We provide evaluation details in our supplementary.

We report the scores in Table [1.](#page-5-3) Quantitatively, our method outperforms the baselines across all metrics by a significant margin. In particular, compared to the result obtained by the best prior work for each metric, our model improves CD-3D, CD-4D and CD-M by respectively 21%, 46% and 45%. In addition, it is an order of magnitude faster at inference (3min vs 15–45min for prior works).

Qualitative comparison on Consistent4D. We compare our method on videos from the standard evaluation set of Consistent4D [\[14\]](#page-8-0) and representative examples are shown in Figure [5.](#page-6-1) We observe that LIM and DreamMesh4D exhibit

reduced shape fidelity with softer details and visible artifacts. Although V2M4 and ShapeGen4D recover sharper details, they also produce artifacts and partial temporal drift. On the contrary, our model yield high-quality meshes with a better temporal coherence and a much stronger motion fidelity. It is worth noting that it does so while being significantly faster. Other examples are shown in our supplementary.

# <span id="page-6-0"></span>4.2. Additional results

Real-world videos. In Figure [6,](#page-7-0) we demonstrate our method's robustness by showing qualitative results on realworld videos from DAVIS [\[27\]](#page-8-28), where a segmentation model was used to isolate the foreground object. Although our model was only trained on synthetic data, it is able to perform accurate 4D reconstructions on these challenging examples. In particular, it is able to capture large-scale motions like a jumping horse (top right) as well as more subtle movements like a bear walking (bottom right). Notably, our rig-free approach excels in handling complex, multi-part animations, as illustrated by the aquarium scene (bottom left).

Motion transfer. In Figure [4,](#page-5-0) we show our model's ability to transfer motion from a given video representing an object A to a different 3D object B, without explicitly being trained for this task. In particular, we found this process to work well when the semantic correspondences between object A and object B can be established. For example, our model enables us to seamlessly animate a 3D dragon using a casual video of a flying bird (bottom).

<span id="page-7-3"></span><span id="page-7-0"></span>![](_page_7_Figure_0.jpeg)

Figure 6. Qualitative results on real videos from DAVIS [\[27\]](#page-8-28). Even in these challenging scenarios, our model produces accurate animated 3D meshes, thus demonstrating its ability to handle complex motions, multiple objects and occlusions. See supplemental for more results.

<span id="page-7-1"></span>Table 2. Ablation study. We evaluate some key components, namely our stage I, both stages I and II, and the pretrained model we started from by using Craftsman [\[16\]](#page-8-4) instead of TripoSG [\[17\]](#page-8-7).

| Ablation type         | CD-3D↓ | CD-4D↓ | CD-M↓ |
|-----------------------|--------|--------|-------|
| Full model            | 0.050  | 0.069  | 0.137 |
| w/o stage II          | 0.050  | 0.069  | –     |
| w/o stage I & II      | 0.050  | 0.187  | –     |
| w/ Craftsman backbone | 0.072  | 0.117  | 0.216 |

Ablation study. Table [2](#page-7-1) analyzes the influence of some of our key design choices. First, we remove stage II and thus evaluate the performance of stage I only, which is not able to produce animated 3D meshes. Interestingly, stage II preserves the 3D reconstruction quality while allowing us to predict an animated mesh. Second, we remove both stages; this amounts to running the image-to-3D model (TripoSG) on each frame independently. This experiment shows that stage I is the critical component to obtain accurate 4D reconstructions. Besides, thanks to our minimal modifications, it is worth noting that adding both stages does not harm the 3D reconstruction quality of our backbone. Finally, we replace the TripoSG [\[17\]](#page-8-7) backbone with Craftsman [\[16\]](#page-8-4) and observe that the method still achieves competitive performances. We provide additional analysis in our supplementary material.

## 5. Conclusion

We presented ActionMesh, a fast, feed-forward generative model that produces animated 3D meshes that are topologyconsistent and rig-free, directly from diverse inputs. Our key insight relies on temporal 3D diffusion: we extend pretrained 3D diffusion models with a temporal axis to generate a sequence of synchronized shape latents, and then use a temporal 3D autoencoder to translate these shapes into deformations of a reference mesh, yielding an animation with consistent topology. This delivers high-fidelity

<span id="page-7-2"></span>![](_page_7_Picture_7.jpeg)

Figure 7. Limitations. Typical failure cases arise for videos with topological changes (left) and regions that are occluded either on the reference frame (middle) or during the motion (right).

shape and motion in 3 minutes, enabling rapid iteration and seamless downstream use in texturing and retargeting. We report state-of-the-art geometric accuracy and temporal consistency, demonstrating that our model is a simple, general, and practical path to production-ready animated 3D meshes.

Limitations and directions. We highlight two typical failure cases in Figure [7](#page-7-2) that we discuss next:

- *Topological changes (left).* We assume fixed connectivity and thus changes in topology cannot be modeled. Rather than explicit mesh surgery, a promising direction is to enable topology-aware latent updates that instantiate, fuse, or remove local parts without manual connectivity edits.
- *Strong occlusions (middle, right).* Although our model is able to hallucinate parts that are not visible, it sometimes fail at reconstructing occluded regions, in particular when they are missing from the reference frame or when they disappear during a complex motion.

ActionMesh's ability to lift everyday video into 4D unlocks learning geometric motion priors directly from videos. We believe this closes the loop between large-scale video corpora and mesh-native reasoning, paving the way for richer, more generalizable 4D understanding and generation.

#