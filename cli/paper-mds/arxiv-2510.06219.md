# <span id="page-0-0"></span>HUMAN3R: EVERYONE EVERYWHERE ALL AT ONCE

Yue Chen<sup>1</sup> Xingyu Chen<sup>1</sup><sup>∗</sup> Yuxuan Xue<sup>2</sup> Anpei Chen<sup>1</sup> Yuliang Xiu<sup>1</sup>† Gerard Pons-Moll<sup>2</sup>,<sup>3</sup> <sup>1</sup>Westlake University <sup>2</sup>Uni of Tubingen, T ¨ ubingen AI Center ¨ <sup>3</sup>Max Planck Institute for Informatics ∗ Project Lead †Corresponding Author

![](_page_0_Picture_4.jpeg)

Figure 1: Given a stream of RGB images as input, Human3R enables human-scene reconstruction in an online, continuous manner, estimating global multi-person meshes, camera parameters, and dense scene geometry with each incoming frame in real time.

### ABSTRACT

We present Human3R, a unified, feed-forward framework for online 4D humanscene reconstruction, in the world frame, from casually captured monocular videos. Unlike previous approaches that rely on multi-stage pipelines, iterative contactaware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies (*"everyone"*), dense 3D scene (*"everywhere"*), and camera trajectories in a single forward pass (*"all-at-once"*). Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates heavy dependencies and iterative refinement. After being trained on the relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it achieves superior performance with remarkable efficiency: it reconstructs multiple humans in a one-shot manner, along with 3D scenes, in one stage, in real-time (15 FPS) with a low memory footprint (8 GB). Extensive experiments demonstrate that Human3R delivers state-of-the-art or competitive performance across tasks, including global human motion estimation, local human mesh recovery, video depth estimation, and camera pose estimation, with a single unified model. We hope that Human3R will serve as a simple yet strong baseline, which can be easily adapted for downstream applications. Code available in [fanegg.github.io/Human3R](https://fanegg.github.io/Human3R).

### 1 INTRODUCTION

Humans do not exist in isolation but constantly move in, interact with, and manipulate the world around us. Thus, understanding human behaviors requires putting them within a 3D world context, ideally in an online manner, as indicated in Fig. [2.](#page-1-0) In the field of 3D vision, this necessitates the 3D reconstruction of both global human motions and the surrounding scene from visual data [\[30\]](#page-11-0), which is challenging, but fundamental for various downstream applications, including AR/VR, autonomous navigation, humanoid policy learning, and human-scene interaction.

Prior global human motion estimators typically follow one of two strategies: 1) *directly* estimating the global human motions aided with learned motion priors [\[72,](#page-13-0) [110\]](#page-15-0); 2) transforming human motion to world coordinates with SLAM-based [\[91\]](#page-14-0) estimated global camera [\[44,](#page-11-1) [48,](#page-12-0) [80,](#page-14-1) [82,](#page-14-2) [89,](#page-14-3) [99,](#page-15-1) [109\]](#page-15-2). Considering the surrounding 3D scene, which is crucial for contextualizing human actions, recent advances attempt to jointly reconstruct 3D humans, scene, and cameras, either from multi-view images [\[16,](#page-10-0) [56,](#page-12-1) [74\]](#page-13-1) or monocular videos [\[53\]](#page-12-2).

<span id="page-1-1"></span>However, these methods have two main limitations: 1) Multi-stage/model/shot: They [\[53,](#page-12-2) [56\]](#page-12-1) reconstruct the scene and humans separately, then jointly refine them under contact constraints. The entire pipeline takes hours. In addition, a top-down multi-person mesh regressor is used, which requires off-the-shelf human detection and human tracking models [\[15,](#page-10-1) [41,](#page-11-2) [50,](#page-12-3) [71,](#page-13-2) [73,](#page-13-3) [106\]](#page-15-3) to crop and associate each person before feeding into the single-person mesh regressor [\[31,](#page-11-3) [99\]](#page-15-1), thus the inference speed considerably drops for images with multiple people.

<span id="page-1-0"></span>![](_page_1_Figure_2.jpeg)

Figure 2: Human behaviors (i.e., grocery shopping) become clearer when viewed within their surrounding environment.

2) Heavy dependencies: Apart from the modules mentioned above, numerous off-the-shelf dependencies are needed to preprocess the input images, including but not limited to metric depth estimators [\[4\]](#page-9-0), generic 3D reconstruction models [\[24,](#page-10-2) [46,](#page-12-4) [91,](#page-14-0) [98\]](#page-15-4) to obtain the 3D scene pointcloud, camera pose and intrinsics. Both limitations hinder real-time online inference, end-to-end learning, effortless deployment, and scalability to long sequences. We seek a unified one-stop solution.

We introduce Human3R, an *all-at-once* model for 4D human-scene reconstruction. The term "*all-atonce*" reflects several key aspects: 1) One Model: A unified model jointly reasons about humans, scene, and camera, rather than relying on separate off-the-shelf models for each component. 2) One Stage: In contrast to prior work with iterative refinement, our method runs in an online fashion, operates on streaming video at real-time speed (15 FPS) without compromising accuracy. 3) One Shot: With a bottom-up multi-person SMPL-X regressor, our model can reconstruct multiple persons in a single forward pass. 4) One GPU, One Day: Our model is parameter efficient, requiring only one day of training on a single NVIDIA 48GB GPU, still yielding state-of-the-art performance.

The main challenge in building such a unified model lies in the lack of large-scale video datasets with reliable annotations of global human motion, 3D scene, and camera pose. Existing real datasets [\[21,](#page-10-3) [33,](#page-11-4) [39,](#page-11-5) [94\]](#page-14-4) are limited in scale, while synthetic ones, like BEDLAM [\[5\]](#page-9-1), are limited in scene variations. Our key idea is to leverage the strong spatiotemporal priors [\[13,](#page-9-2) [27,](#page-10-4) [108\]](#page-15-5) learned by a 4D reconstruction foundation model [\[97\]](#page-15-6), and extend it through minimal tuning on a relatively small-scale human-scene dataset, achieving both data and parameter efficiency. This approach enables us to advance from point-only reconstruction to the joint reconstruction of dense scene point clouds and sequential SMPL-X body meshes [\[64\]](#page-13-4) for multiple individuals in the scene.

Specifically, we build upon CUT3R [\[97\]](#page-15-6), a recurrent 4D reconstruction foundation model for online metric-scale reconstruction, which maintains a persistent internal state that encodes *everywhere and everyone*, and incrementally updates it with new observations. We finetune CUT3R via visual prompt tuning (VPT) [\[35\]](#page-11-6), with minimal learnable parameters prepended into the input space while the entire CUT3R backbone is kept frozen. BEDLAM [\[5\]](#page-9-1) serves as our training data, which is small-scale yet high-quality, with 6k sequences featuring 3D scene depth, camera poses, and SMPL-X meshes of multiple persons in the world coordinates. Instead of naively prepending random initialized learnable tokens as visual prompts [\[35\]](#page-11-6), we detect the human head tokens from CUT3R's image feature, complement it with human prior tokens [\[3\]](#page-9-3) learned from human-specific datasets, and project them to *human prompts* using a learnable MLP, as shown in Fig. [4.](#page-4-0)

Our proposed *human prompts* are highly informative, as the head is the most discriminative keypoint on human bodies [\[3,](#page-9-3) [116\]](#page-16-0). As anchors (i.e., SMPL-X queries), these human prompts provide strong spatial priors for localizing and reconstructing the full human body. They self-attend to image tokens for spatial whole-body information aggregation, and cross-attend to the persistent internal state to make 3D human estimates scene-aware. Remarkably, Fig. [8](#page-7-0) shows that the 3D scene reconstruction is also improved after finetuning for human reconstruction, demonstrating the mutual benefits of joint reasoning about humans and scene.

Simple yet effective, Human3R leverages the spatial and temporal priors learned by CUT3R to reason about humans, scene, and camera in a unified framework, efficiently processes long sequences with linear computational complexity (8 GB GPU memory footprint, 15 FPS inference speed), and supports scalable sequence length (thousands of frames) beyond training length (4 frames) by simply rolling out the state. Across various 4D tasks — including video depth estimation, camera pose estimation, human mesh recovery, and global human motion estimation — our method achieves superior performance over task-specific baselines while offering a unified and real-time solution.

### <span id="page-2-0"></span>2 RELATED WORKS

Local Human Mesh Recovery. Previous works on human mesh recovery (HMR) primarily focus on estimating the pose and shape parameters of a parametric body model, like SMPL [\[54\]](#page-12-5), SMPL-X [\[64\]](#page-13-4), and GHUM [\[105\]](#page-15-7), *in the camera frame*. Early optimization-based methods fit SMPL model to IMU trajectories [\[94,](#page-14-4) [103\]](#page-15-8) or to 2D landmarks by minimizing reprojection errors [\[6,](#page-9-4) [65\]](#page-13-5). In contrast, learning-based approaches, trained on large-scale image-body pairs, can regress SMPL parameters from images [\[37,](#page-11-7) [59\]](#page-12-6) in a single pass. Progress in this field spans improvements in network architectures [\[25,](#page-10-5) [31,](#page-11-3) [52,](#page-12-7) [111\]](#page-15-9), training and testing paradigms [\[19,](#page-10-6) [45,](#page-11-8) [76\]](#page-13-6), kinematics designs [\[47,](#page-12-8) [49\]](#page-12-9), camera models [\[43,](#page-11-9) [62\]](#page-12-10), datasets [\[5,](#page-9-1) [25,](#page-10-5) [29,](#page-10-7) [36,](#page-11-10) [63\]](#page-13-7), expressive body models [\[18,](#page-10-8) [28,](#page-10-9) [49,](#page-12-9) [64,](#page-13-4) [112\]](#page-16-1), temporal consistency [\[17,](#page-10-10) [38,](#page-11-11) [42\]](#page-11-12), and etc. For multi-person scenarios, most prior works adopt a top-down multi-stage approach: detect and crop each person before running single-person HMR. This is computationally expensive, scales poorly with more people, and often fails in crowded scenes due to occlusion and truncation. To overcome this, bottom-up methods [\[3,](#page-9-3) [87,](#page-14-5) [88,](#page-14-6) [100\]](#page-15-10) recover multiple human meshes from a full image in one-shot scheme. Multi-HMR, for example, finetunes DINOv2 [\[60\]](#page-12-11) on synthetic datasets [\[5,](#page-9-1) [63\]](#page-13-7), and achieves strong performance. Our goal is even more ambitious: to reconstruct both the 3D scene and multiple humans *in the world frame* from monocular videos, using one unified model, in one forward pass, and in real-time.

Global Human Motion Estimation. Reconstructing world-grounded humans from long video sequences is an ill-posed problem, typically requiring additional priors or constraints. GLAMR [\[110\]](#page-15-0) leverages the learned motion prior HuMoR [\[72\]](#page-13-0) to infill occluded human motions and directly predict global trajectories from them. With SLAM (Simultaneous Localization and Mapping) [\[91\]](#page-14-0), world-frame camera poses can be estimated, allowing local human meshes – recovered via HMR – to be transformed into the world frame [\[48,](#page-12-0) [109\]](#page-15-2). TRAM [\[99\]](#page-15-1) robustifies and metrifies SLAM's camera estimation via masking the dynamic regions and estimating metric depth via ZoeDepth [\[4\]](#page-9-0), which then serve as a reference frame to recover the

![](_page_2_Figure_4.jpeg)

Figure 3: Multi-stage vs. One-stage.

global human motion. GVHMR [\[80\]](#page-14-1) introduces gravity and view-in direction constraints to further stabilize global human motions. Beyond these offline solutions, several online methods [\[82,](#page-14-2) [89\]](#page-14-3) recurrently reconstruct global human meshes, maintaining consistently low memory and computation costs as the number of input frames increases. However, even excluding the SLAM step, most of these approaches still depend on multiple off-the-shelf estimators – such as human detection [\[48,](#page-12-0) [80,](#page-14-1) [82,](#page-14-2) [99,](#page-15-1) [109,](#page-15-2) [110\]](#page-15-0), tracking [\[48,](#page-12-0) [80,](#page-14-1) [82,](#page-14-2) [99,](#page-15-1) [109,](#page-15-2) [110\]](#page-15-0), segmentation [\[99\]](#page-15-1), 2D keypoint detection [\[48,](#page-12-0) [80,](#page-14-1) [82,](#page-14-2) [109,](#page-15-2) [110\]](#page-15-0), optical flow [\[89\]](#page-14-3), camera-frame HMR [\[48,](#page-12-0) [99,](#page-15-1) [109,](#page-15-2) [110\]](#page-15-0), and etc. Synchronization barriers between these branches often lead to cumulative errors and high computational overhead. In contrast, Human3R is an *all-in-one* model that not only online recovers human motions and root trajectories in the world frame, but also simultaneously reconstructs the surrounding 3D scene and estimates camera motions – an versatile framework not explored in prior works.

Human-Scene Reconstruction. Existing methods for joint human-scene 3D reconstruction typically perform global optimization over camera poses, pre-reconstructed scenes [\[24,](#page-10-2) [51,](#page-12-12) [79,](#page-13-8) [98\]](#page-15-4) (please checkout related work about generic 3D reconstruction in Sec. [B](#page-19-0) of *Sup.Mat.*), and SMPL mesh parameters inferred from multi-view images [\[56,](#page-12-1) [66\]](#page-13-9), often regularized by learned motion priors [\[2,](#page-9-5) [53,](#page-12-2) [115\]](#page-16-2). Recently, optimization-free approaches have emerged: HAMSt3R [\[74\]](#page-13-1), for example, jointly reconstructs the scene and DensePose [\[32\]](#page-11-13) from multi-view images in a feed-forward manner, then fits SMPL meshes to the DensePose outputs. The most relevant work, JOSH3R [\[53\]](#page-12-2), jointly reconstructs scene and human meshes from monocular videos with dynamic humans, but depends on camera-frame human meshes, detection, segmentation, and tracking, limiting scalability and efficiency. We eliminate all these dependencies, resulting in a lightweight yet unified model that directly predicts metric-scale dense scenes, global human motions, and camera poses from monocular video in a single forward pass. This unified approach distinguishes our method from previous works and opens up new possibilities for real-time applications in humanoid policy learning, autonomous navigation, and human-robot interaction.

### <span id="page-3-1"></span>3 Methods

Our approach operates on a continuous stream of images in an online manner. At each timestep t, given an input image  $\mathbf{I}_t \in \mathbb{R}^{W \times H \times 3}$ , our goal is to estimate: 1) a set of N human meshes  $\{\mathbf{M}_t^n \in \mathbb{R}^{V \times 3}\}_{n=1}^N$  in the world coordinate system, where each  $\mathbf{M}_t^n$  is parameterized by the SMPL-X body model with V=10,475 vertices and K=54 joints; 2) the camera extrinsic pose  $\mathbf{T}_t \in \mathbb{R}^{3 \times 4}$ , and intrinsic  $\mathbf{C}_t \in \mathbb{R}^{3 \times 3}$ ; 3) the canonical point cloud  $\mathbf{X}_t \in \mathbb{R}^{W \times H \times 3}$ . Our feedforward inference operates online in real time. We first introduce preliminaries of the 3D human parametric model and the 4D reconstruction foundation model CUT3R [97] in Section 3.1. Then, in Section 3.2, we describe our proposed Human3R, which fine-tunes CUT3R to regress SMPL-X parameters for multiple 3D human bodies.

#### <span id="page-3-0"></span>3.1 PRELIMINARIES

**Human Mesh Representation – SMPL-X [64].** We represent the 3D human body with the SMPL-X [54, 64], which is a low-dimensional parametric model of the human body mesh. Given the parameters of the local human pose (relative axis-angle rotations)  $\boldsymbol{\theta} \in \mathbb{R}^{52 \times 3}$ , body shape  $\boldsymbol{\beta} \in \mathbb{R}^{10}$ , facial expression  $\boldsymbol{\alpha} \in \mathbb{R}^{10}$ , and global human root transformation  $\mathbf{P} = [\mathbf{R} \mid \mathbf{t}] \in \mathrm{SE}(3)$  parametrized by global orientation  $\mathbf{R} \in \mathrm{SO}(3)$  and global translation  $\mathbf{t} \in \mathbb{R}^3$ , it outputs an expressive 3D human mesh  $\mathbf{M}^n_t \in \mathbb{R}^{V \times 3}$ , with V = 10,475 vertices. For brevity, we omit the timestep subscript t and the id superscript t, as  $\mathbf{M}^n_t \to \mathbf{M}$ :

$$\begin{aligned} \mathbf{M} &= \mathrm{SMPL-X}(\boldsymbol{\theta}, \boldsymbol{\beta}, \boldsymbol{\alpha}, \mathbf{P}) \\ \mathbf{P} &= \mathbf{TP}^{\mathrm{cam}} \end{aligned} \tag{1}$$

where the global human root transformation  $\mathbf{P}$ , in the world frame, is decomposed into the camera pose  $\mathbf{T}$  and the local root transformation  $\mathbf{P}^{\text{cam}}$  in the camera frame.

**4D Reconstruction Foundation Model – CUT3R [97].** To overcome the scarcity of world-grounded 4D human-scene datasets, we exploit the 4D reconstruction foundation model CUT3R [97], which is 4D-aware, and encodes rich 4D priors of real-world dynamics, including both scene (*everywhere*) and human (*everyone*), learned from large-scale 3D point cloud datasets. However, instead of explicitly separating the unstructured point clouds of humans from the scene, Human3R directly reads out global human bodies.

CUT3R performs recurrent reconstruction of metric-scale point maps (pixel-aligned point clouds in the world coordinate system) and camera poses in an online fashion, maintaining a fixed-size memory state that encodes *everything* that camera captures. This state enables the retrieval of past observations, while being continuously updated with new observations. Specifically, to transform a current image  $\mathbf{I}_t$  into pixel-aligned point maps, the input image is encoded into a set of image tokens  $\mathbf{F}_t \in \mathbb{R}^{(h \times w) \times c}$  through the ViT image tokenizer [23]:  $\mathbf{F}_t = \operatorname{Encoder}(\mathbf{I}_t)$ . The image tokens then interact with the state in the following formulation:

$$[\mathbf{F}_t', \mathbf{z}_t'], \mathbf{S}_t = \text{Decoders}([\mathbf{F}_t, \mathbf{z}], \mathbf{S}_{t-1})$$
 (2)

where the init state representation is represented as a set of tokens  $\mathbf{S}_0 \in \mathbb{R}^{768 \times 768}$ , which are learnable parameters and are shared by all scenes. As the set of image tokens  $\mathbf{F}_t$  is fed into the decoder, the previous state  $\mathbf{S}_{t-1}$  is updated with new observations to produce an updated state  $\mathbf{S}_t$ , which encodes the spatial and temporal history of the scene, namely "context". Then, through the decoder, the image token  $\mathbf{F}_t$  and camera token  $\mathbf{z}_t$ , attend with the context in current state  $\mathbf{S}_t$ , will be refined as  $\mathbf{F}_t'$  and  $\mathbf{z}_t'$ . The camera token, designed to capture the image-level ego motion related to the scene, is prepended to the image tokens and is initialized as a learnable parameter  $\mathbf{z}$ . This bidirectional state-token interaction is implemented using two interconnected transformer decoders [98, 101, 102].

After the state-token interaction, the corresponding pixel-aligned metric scale (i.e., meters) 3D pointmaps in the camera and world coordinate systems are extracted via dense prediction head [70]:  $\mathbf{X}_t^{\text{cam}} = \text{Head}_{\text{cam}}(\mathbf{F}_t')$ ,  $\mathbf{X}_t^{\text{world}} = \text{Head}_{\text{world}}(\mathbf{F}_t', \mathbf{z}_t')$ . The camera pose  $\mathbf{T}_t$  is then regressed from camera tokens through an MLP network:  $\mathbf{T}_t = \text{Head}_{\text{pose}}(\mathbf{z}_t')$ , and the camera intrinsic  $\mathbf{C}_t$  is solved using Weiszfeld [68] algorithms with predicted pointmaps, respectively.

<span id="page-4-2"></span><span id="page-4-0"></span>![](_page_4_Figure_1.jpeg)

Figure 4: **Method Overview**. Human3R enables online human-scene reconstruction from video streams. Each frame is encoded into image tokens, with patch-level detection. Each detected head token, concatenated with a human prior token from Multi-HMR [3] ViT-DINO feature, is projected into a *human prompt*. The *human prompts* serve as discriminative human-ID queries for the decoder: they self-attend with image tokens to aggregate spatial whole-body information and cross-attend with the scene state to retrieve temporally consistent human tokens within the 3D scene context. Only human-related layers are fine-tuned, other parameters remain frozen and are initialized from CUT3R [97].

#### <span id="page-4-1"></span>3.2 Human3R

**One-stage Global Human-Scene Reconstruction.** To preserve the rich 4D priors encoded by CUT3R, we adopt parameter-efficient visual prompt tuning (VPT) [35] for fine-tuning. Specifically, we introduce a small set of trainable parameters – prepended as visual prompts into the input space – to enable the readout of global human meshes, while keeping the entire CUT3R backbone frozen.

Unlike standard VPT, where additional parameters are randomly initialized learnable tokens, we instead detect human head tokens and transform them into *human prompts* using learnable projection layers. Specifically, we follow previous work [3] to detect the human head (defined by the head joint of SMPL-X model) as the primary keypoint of human. For each

![](_page_4_Figure_6.jpeg)

Figure 5: Detection and Segmentation.

patch index  $(i,j) \in \{1,\dots,h\} \times \{1,\dots,w\}$ , we predict whether the patch  $\mathbf{u}^{i,j}$  contains the primary keypoint by computing a confidence score from the associated image feature token  $\mathbf{F}^{i,j} \in \mathbb{R}^c$  using an MLP followed by a sigmoid activation  $\sigma(\cdot)$ , formulated as  $s^{i,j} = \sigma\left(\mathrm{MLP}_{\mathrm{head}}(\mathbf{F}^{i,j})\right)$ . We apply a threshold  $\tau$  on  $s^{i,j}$  to collect detected head token indexes, denoted as  $\left\{\mathbf{u}^{i,j} \mid s^{i,j} \geq \tau\right\}_n$ . We then predict the human mesh parameters  $\mathbf{Y}_t = \{(\theta, \beta, \alpha, \mathbf{P}^{\mathrm{cam}})_t\}_n$  for all people with detected head tokens  $\mathbf{F}^{\mathbf{u}}_t = \{\mathbf{F}^{i,j}_t \mid (i,j) \in \{\mathbf{u}_t\}_n\}$  in parallel:

$$\mathbf{H}_{t} = \frac{\text{Head}_{\text{projection}}(\mathbf{F}_{t}^{\mathbf{u}})}{[\mathbf{F}_{t}', \mathbf{z}_{t}', \mathbf{H}_{t}'], \mathbf{S}_{t} = \frac{\text{Decoders}([\mathbf{F}_{t}, \mathbf{z}, \mathbf{H}_{t}], \mathbf{S}_{t-1})}{\mathbf{Y}_{t} = \frac{\text{Head}_{\text{human}}(\mathbf{H}_{t}')}$$
(3)

where human prompts  $\mathbf{H}_t$  is transformed from detected head tokens  $\mathbf{F}_t^{\mathbf{u}}$  via the projection MLP, and the SMPL-X parameters  $\mathbf{Y}_t$  are predicted by the human MLP from the refined human token  $\mathbf{H}_t'$ .  $\mathbf{H}_t$  is inserted into the input space of the decoder. The colors • and • indicate learnable and frozen parameters, respectively. During fine-tuning, only the human-related MLP layers are updated, while all other parameters remain frozen. The human prompts serve as discriminative human ID queries: they self-attend with image tokens to aggregate spatial whole-body information and cross-attend with the scene state to retrieve temporal SMPL-X mesh parameters within the 3D scene context.

**Human Prior.** In practice, we found that CUT3R, trained on large-scale scene-centric datasets, lacks detailed human priors, leading to suboptimal performance in reconstructing fine-grained human poses and shapes. Thus, we enhance the head tokens  $\mathbf{F^u}$  with extra human-specific features from a human-centric image encoder. Particularly, we use another image tokenizer, the Multi-HMR [3] ViT image encoder, denoted as  $\operatorname{Encoder_{HMR}}$ , which fully fine-tuned the pretrained DINO [10, 60] on human-specific datasets. Same as previous index-based query, we still use  $\{\mathbf{u}\}_n$  to obtain the corresponding Multi-HMR ViT image tokens  $\mathbf{F}_{\text{HMR}} = \operatorname{Encoder_{HMR}}(\mathbf{I})$ , to produce  $\mathbf{F}_{\text{HMR}}^{\mathbf{u}} = \{\mathbf{F}_{\text{HMR}}^{i,j} \mid (i,j) \in \{\mathbf{u}\}_n\}$ , which are subsequently concatenated with CUT3R head tokens  $\mathbf{F^u}$  and translated into human prompts by the projection MLP as:  $\mathbf{H} = \operatorname{Head_{projection}}(\mathbf{F^u} \oplus \mathbf{F_{HMR}^u})$ , where  $\oplus$  denotes concatenation along the channel axis. Notably,  $\operatorname{Encoder_{HMR}}$  is frozen during training. Concatenating Multi-HMR and CUT3R head tokens injects detailed human priors for improved body pose and shape prediction. And with additional *training-free* designs, Human3R also supports human segmentation and tracking.

<span id="page-5-0"></span>**Human Segmentation and Tracking.** For segmentation, we predict whether each patch (i, j) contains human parts by generating a score vector  $\mathbf{m}^{i,j} \in \mathbb{R}^{(16 \times 16) \times 1}$  from the corresponding image token  $\mathbf{F}^{i,j} \in \mathbb{R}^c$ . This is achieved by passing  $\mathbf{F}^{i,j}$  through an MLP, applying a sigmoid activation, and then using pixel shuffle [81] to produce a pixel-aligned

![](_page_5_Picture_2.jpeg)

dense mask:  $\mathbf{m}^{i,j} = \mathtt{PixelShuffle}\left(\sigma\left(\mathrm{MLP_{mask}}(\mathbf{F}^{i,j})\right)\right)$ . We perform human tracking by leveraging the discriminative features encoded in the refined human token  $\mathbf{H}'$ , which encapsulates both human identity and human parameters. This enables us to formulate human tracking as a feature matching problem [77], where tracklet association is achieved by matching the refined tokens across timesteps. We maintain a human token tracklet [69] indexed by  $\mathcal{A} = \{1, \ldots, M\}$  after each step of the online processing, which allows us to build a memory bank for all observed humans, and derive soft assignment matrix  $\mathbf{A} \in [0,1]^{M \times N}$  for current detections indexed by  $\mathcal{B} = \{1, \ldots, N\}$ . To estimate the likelihood of a given tracklet-detection pair, we use the pairwise L2 distance  $\mathbf{D}_{m,n} = ||\mathbf{H}^{m'} - \mathbf{H}^{n'}||_2$ ,  $\forall (m,n) \in \mathcal{A} \times \mathcal{B}$  to obtain the cost matrix  $\mathbf{D} \in \mathbb{R}^{M \times N}$ . To suppress unmatched human tokens, we augment the cost  $\mathbf{D}$  to  $\overline{\mathbf{D}} \in \mathbb{R}^{(M+1) \times (N+1)}$  by appending a new row and column dustbin with a threshold, so that unmatched human tokens are explicitly assigned to it. The assignment with dustbin  $\overline{\mathbf{A}}$  can be solved by optimal transport [67] with the Sinkhorn algorithm [20] to minimize the total  $\cos \sum_{m,n} \overline{\mathbf{D}}_{m,n} \overline{\mathbf{A}}_{m,n}$  under the constraints of  $\overline{\mathbf{A}} \mathbf{1}_{N+1} = \mathbf{a}$  and  $\overline{\mathbf{A}}^{\top} \mathbf{1}_{M+1} = \mathbf{b}$ , where  $\mathbf{a} = [\mathbf{1}_M^{\top} N]^{\top}$  and  $\mathbf{b} = [\mathbf{1}_N^{\top} M]^{\top}$ , denote the number of expected matches for each human token and dustbin in  $\mathcal{A}$  and  $\mathcal{B}$ .

**Training Strategy.** We finetune CUT3R on a synthetic dataset, BEDLAM [5], which is small-scale yet high-quality, with 6k sequences featuring 3D scene depth, camera poses, and SMPL-X meshes [64] of multiple persons in the world coordinates. Following CUT3R and MASt3R, we apply a confidence-aware 3D regression loss  $\mathcal{L}_{\text{pointmap}}$  to the metric-scale pointmaps, as well as a camera pose loss  $\mathcal{L}_{\text{pose}}$  to the ground-truth camera poses. This helps prevent CUT3R from forgetting the rich spatial and temporal priors learned from large-scale 3D scene datasets. To readout human from CUT3R, we follow Multi-HMR to minimize a binary cross-entropy loss  $\mathcal{L}_{\text{detection}}$  on  $s^{i,j}$ , L1 regression losses  $\mathcal{L}_{\text{smpl}}$  to human parameter  $\mathbf{Y}_t$ ,  $\mathcal{L}_{\text{mesh}}$  to explicit human meshes, and reprojection loss  $\mathcal{L}_{\text{reproj.}}$ . With our efficient human prompt tuning protocol, Human3R requires just one day of training on a single NVIDIA 48GB GPU, and still achieves state-of-the-art performance. Please checkout more training details in Sec. C of Sup.Mat.

Test-Time Sequence Length Adaptation. Trained with sequences of only 4 images, we observe that performance of Human3R degrades when the inference sequence length exceeds the training context. This is a common issue for RNN-based methods [9, 14, 95, 104], including CUT3R [97], where the state tends to forget earlier frames, resulting in significant performance drops as the number of input views increases. To address this limitation and support longer sequence, we adopt TTT3R [12], which parameterizes the state S as a fast weight [78] and updates it using gradient descent:  $\mathbf{S}_t = \mathbf{S}_{t-1} - \beta_t \nabla(\mathbf{S}_{t-1}, \mathbf{F}_t, \mathbf{z})$ , where  $\nabla(\mathbf{S}_{t-1}, \mathbf{F}_t, \mathbf{z})$  denotes the gradient function and  $\beta_t$  is the learning rate. Intuitively, this Test-Time Training (TTT) [90] procedure adaptively encodes the current observation into the memory state using a dynamic learning rate, enabling online adaptation. This approach effectively balances the retention of historical context with the integration of new observations. We follow TTT3R to use the spatial average of the attention values as a closed-form update rule for online associative recall in test time, and formulate the state update as:  $\mathbf{S}_t = \mathbf{S}_{t-1} - \beta_t \nabla(\mathbf{S}_{t-1}, \mathbf{F}_t, \mathbf{z}, \mathbf{H}_t)$ . Inspired by the correlation between length generalization and unexplored state distributions [75], we further propose a state reset process: the state is reset every 100 frames, using the global camera pose as a cue to align the resulting chunks.

### 4 EXPERIMENTS

We unfold the validation of Human3R and the baselines on human mesh recovery in the camera coordinates (Sec. 4.1) and the world coordinates (Sec. 4.2) respectively, and then compare our model with current state-of-the-art genetic 3D reconstruction methods in camera pose estimation and video depth estimation (Sec. 4.3). We also analyze the components of Human3R in Sec. 4.4.

<span id="page-6-4"></span><span id="page-6-2"></span>

|             |                 |           |                |                | 3D          | PW (14)           |                  | EMDB-1 (24) |                   |                  |  |  |
|-------------|-----------------|-----------|----------------|----------------|-------------|-------------------|------------------|-------------|-------------------|------------------|--|--|
| Category    | Method          | Crop-free | Detection-free | Intrinsic-free | PA-MPJPE↓   | $MPJPE\downarrow$ | $PVE \downarrow$ | PA-MPJPE↓   | $MPJPE\downarrow$ | $PVE \downarrow$ |  |  |
|             | CLIFF [52]      | Х         | X              | Х              | 43.0        | 69.0              | 81.2             | 68.3        | 103.3             | 123.7            |  |  |
|             | HMR2.0a [31]    | ×         | ×              | ✓              | 44.4        | 69.8              | 82.2             | 61.5        | 97.8              | 120.0            |  |  |
| Multi-stage | TokenHMR [25]   | ×         | ×              | ✓              | 44.3        | 71.0              | 84.6             | 55.6        | 91.7              | 109.4            |  |  |
|             | CameraHMR [62]  | ×         | ×              | ✓              | 38.5        | 62.1              | 72.9             | 43.7        | 73.0              | 85.4             |  |  |
|             | NLF [76]        | ×         | ×              | ×              | <u>37.3</u> | 60.3              | 71.4             | 41.2        | 69.6              | 82.4             |  |  |
|             | PromptHMR [100] | 1         | X              | X              | 36.6        | 58.7              | 69.4             | 41.0        | 71.7              | 84.5             |  |  |
| One-stage   | BEV [88]        | 1         | 1              | ✓              | 46.9        | 78.5              | 92.3             | 70.9        | 112.2             | 133.4            |  |  |
|             | Multi-HMR [3]   | 1         | ✓              | X              | <u>45.9</u> | 73.1              | 87.1             | 50.1        | 81.6              | <u>95.7</u>      |  |  |
|             | Human3R         | /         | ✓              | ✓              | 44.1        | 71.2              | 84.9             | 48.5        | 73.9              | 86.0             |  |  |

Table 1: Evaluation of local human mesh reconstruction on 3DPW [94] and EMDB-1 [39] datasets.

<span id="page-6-3"></span>

|          |              | Preprocessed Input (✓ = not required) |          |            |        |      |       |          | Output      |            |       | EM          | DB-2 (24) |        | RICH (24)  |          |         |
|----------|--------------|---------------------------------------|----------|------------|--------|------|-------|----------|-------------|------------|-------|-------------|-----------|--------|------------|----------|---------|
| Category | Method       | Detection                             | Tracking | LocalHuman | Camera | Mask | Depth | Contact  | GlobalHuman | CameraPose | Scene | WA-MPJPE↓   | W-MPJPE ↓ | . RTE↓ | WA-MPJPE ↓ | W-MPJPE. | ↓ RTE ↓ |
|          | GLAMR [110]  | Х                                     | Х        | Х          | 1      | /    | /     | 1        | 1           | Х          | Х     | 280.8       | 726.6     | 11.4   | 129.4      | 236.2    | 3.8     |
| Offline  | SLAHMR [109] | X                                     | X        | ×          | X      | /    | /     | 1        | 1           | ×          | X     | 326.9       | 776.1     | 10.2   | 132.2      | 237.1    | 6.4     |
|          | COIN [48]    | X                                     | X        | ×          | X      | /    | /     | 1        | 1           | ×          | X     | 152.8       | 407.3     | 3.5    | 169.5      | 254.5    | -       |
|          | GVHMR [80]   | Х                                     | X        | ×          | X      | /    | 1     | 1        | /           | ×          | X     | 111.0       | 276.5     | 2.0    | 78.8       | 126.3    | 2.4     |
|          | TRAM [99]    | Х                                     | X        | ×          | X      | X    | X     | 1        | /           | 1          | X     | <u>76.4</u> | 222.4     | 1.4    | 127.8      | 238.0    | 6.0     |
|          | JOSH [53]    | X                                     | X        | Х          | X      | X    | X     | X        | 1           | ✓          | /     | 68.9        | 174.7     | 1.3    | 89.0       | 132.5    | 3.0     |
| Online   | TRACE [89]   | /                                     | <b>✓</b> | <b>√</b>   | /      | /    | /     | <b>√</b> | /           | X          | Х     | 529.0       | 1702.3    | 17.7   | 238.1      | 925.4    | 610.4   |
|          | WHAM [82]    | X                                     | X        | ×          | X      | /    | /     | 1        | 1           | ×          | X     | 135.6       | 354.8     | 6.0    | 108.4      | 196.1    | 4.5     |
|          | JOSH3R [53]  | Х                                     | Х        | X          | 1      | Х    | 1     | 1        | 1           | /          | 1     | 220.0       | 661.7     | 13.1   | -          | -        | -       |
|          | Human3R      | /                                     | 1        | ✓          | 1      | 1    | 1     | 1        | 1           | ✓          | /     | 112.2       | 267.9     | 2.2    | 110.0      | 184.9    | 3.3     |

Table 2: Evaluation of global human motion estimation on EMDB-2 [39] and RICH [33] datasets.

#### <span id="page-6-0"></span>4.1 LOCAL HUMAN MESH RECONSTRUCTION

We evaluate human pose and shape reconstruction in camera coordinates on 3DPW [94] and EMDB (subset 1) [39], and follow the commonly used local human mesh reconstruction metrics as prior works [3, 100]: mean per-joint position error (MPJPE), Procrustes-aligned per-joint position error (PA-MPJPE), and per-vertex error (PVE) measured in millimeters (mm).

We compare with both multi-stage and one-stage leading methods in Tab. 1. Most multi-stage methods rely on human detection and cropping, processing each detected person individually. Without additional cropping, PromptHMR [100] takes the full image as input and prompt it with bounding-box prompts, and achieves strong performance. Among one-stage models, Multi-HMR [3] eliminates the need for off-the-shelf human detectors, but still requires ground-truth camera intrinsics. BEV [88] removes the dependency on ground-truth intrinsics, aligning with our experimental setting. Our approach surpasses these methods across all metrics, demonstrating substantial performance improvements (10% improvement on MPJPE and PVE on EMDB-1), which we attribute to the spatiotemporal awareness provided by CUT3R as a generic 4D reconstruction model.

#### <span id="page-6-1"></span>4.2 GLOBAL HUMAN MOTION ESTIMATION

We evaluate motion and trajectory estimation accuracy in world coordinates on EMDB (subset 2) [39] and RICH [33], both feature long sequences with ground-truth global human trajectories and meshes. Following previous work [82, 99], we divide each sequence into 100-frame segments and evaluate 3D joint errors using two metrics: W-MPJPE, which aligns the first two frames, and WA-MPJPE, which aligns the entire segment. Both metrics are reported in millimeters (mm). To comprehensively assess trajectory accuracy over long sequences, we additionally report the root translation error (RTE, in %) after rigid alignment (without scaling), normalized by the total displacement.

We compare with both offline and online methods in Tab. 2. Given multiple offline pre-cached conditions, GVHMR [80] and JOSH [53] respectively achieve strong performance on sequences with static cameras (RICH) and long human trajectories (EMDB-2). JOSH3R [53], trained with multi-stage pseudo ground truth from JOSH, removes the need for pre-cached camera poses, depth, contact, and iterative refinement. It enables online prediction of global human trajectories, scene points, and camera poses, but with a  $2\times$  drop in accuracy compared to WHAM and still requires precomputed human detection, segmentation, and meshes in camera coordinates. TRACE [89] takes only RGB video as input, matching our experimental setting, but outputs only global human meshes. In contrast, our method also reconstructs scene geometry and estimates camera poses. In summary, Human3R jointly reconstructs multiple human meshes and trajectories in world space, scene geometry, and camera poses, achieving notable gains (20% lower W-MPJPE and 60% lower RTE against WHAM on EMDB-2), while enabling online inference and end-to-end training. We visualize the global human motion estimation within the dense scene, together with the predicted camera trajectory, in Fig. 7.

<span id="page-7-3"></span><span id="page-7-2"></span>![](_page_7_Figure_1.jpeg)

Figure 7: Qualitative 4D human-scene reconstruction results. Given video captured from a single camera, Human3R performs online reasoning about global human motion, the surrounding environment, and camera poses all at once. The Check our website for video results.

<span id="page-7-0"></span>![](_page_7_Figure_3.jpeg)

Figure 8: Evaluation of generic 3D reconstruction with camera pose estimation on TUM-D [86] and video depth estimation on Bonn [61].

#### <span id="page-7-1"></span>4.3 GENERIC 3D RECONSTRUCTION

**Camera Pose Estimation.** Following prior works [12, 97], we evaluate camera pose estimation accuracy on TUM dynamics [86] dataset with dynamic humans. We report the Absolute Translation Error (ATE) after applying the Sim(3) alignment [92] on the estimated camera trajectory to the ground-truth. We compare with current leading 3D reconstruction foundation models [12, 96, 97, 104, 117].

We include VGGT, an offline method utilizing full attention, as an upper bound for online approaches, since it retains complete historical context without forgetting. VGGT and StreamVGGT rely on full attention, making them relatively slow and prone to running out of memory (OOM). In contrast, CUT3R maintains consistently low GPU usage and enables online inference, but struggles to remember long sequences, resulting in less accurate pose estimation. TTT3R [12] introduces a closed-form state transition rule as a training-free intervention to mitigate the catastrophic forgetting observed in CUT3R. As shown in Fig. 8a, integrating TTT3R with Human3R leads to further improvements in camera pose estimation after *human prompt tuning* compared to the original TTT3R.

Video Depth Estimation. Following common practice [12, 97], we evaluate video depth estimation on Bonn [61] datasets with dynamic humans. We use Absolute Relative Error and  $\delta < 1.25$  (percentage of predicted depths within a 1.25-factor of true depth) as metrics. Metric scale video depth estimation evaluates per-frame depth quality and inter-frame depth consistency without per-sequence scale or shift alignment, which measures the absolute depth accuracy. Fig. 8b presents the quantitative comparison between our method and the online baselines, and still Human3R+TTT3R achieves more acccurate depth estimation over naive TTT3R. We do not plot VGGT [96] and StreamVGGT [117] for the evaluation of the metric depth, as they can only predict the relative depth without metric scale.

By integrating TTT3R and fine-tuning with *human prompt tuning* on human-scene 4D datasets, our approach achieves SOTA human mesh recovery and also slightly improves generic 3D reconstruction. This highlights the mutual benefits of jointly reasoning about humans and scenes.

![](_page_8_Picture_1.jpeg)

<span id="page-8-3"></span>![](_page_8_Picture_2.jpeg)

Figure 9: Comparison with naive CUT3R+Multi-HMR combination in global human motion, 3D scene reconstruction, and camera poses estimation. The colors • and • indicates Prediction and Ground-truth, respectively. Q See Fig. 12 in Sup.Mat. for a zoomed-in visualization.

#### <span id="page-8-0"></span>4.4 ANALYSIS

- 1) Human3R benefits from the 3D awareness of CUT3R. We use the Mean Root Position Error (MRPE) [3] between the predicted and ground-truth pelvis locations to evaluate the quality of spatial location estimation. As shown in Fig. 10, Multi-HMR performance varies when processing images at different aspect ratios, while Human3R performs consistently well without requiring camera intrinsics. The metric-scale 3D scene context guides multi-human recovery by capturing their relative spatial relationships, thereby improving our intrinsic robustness. This enables Human3R to recover coherent 3D humans from intrinsic-agnostic internet images. See more in-the-wild examples on our website.
- 2) Human3R benefits from the human awareness of Multi-HMR. To enhance the details of reconstructed human pose and shape, we introduce Multi-HMR [3] ViT DINO encoder that fine-tuned on human-specific datasets as human prior. As shown in Tab. 3, Human3R reconstructs more fine-grained human pose and shape when injecting human priors in better detail.
- 3) Human3R takes the best of both worlds. Human3R predicts better camera poses and scenes than CUT3R (Fig. 8), better local humans than Multi-HMR (Tab. 1 and Fig. 10), and better global humans than the naive combinations of Multi-HMR and CUT3R (Tab. 3), *all-at-once*.

<span id="page-8-1"></span>![](_page_8_Figure_8.jpeg)

Figure 10: Evaluation of intrinsic robustness. Multi-HMR w/ GT intrinsics and Multi-HMR w/o GT intrinsics are sensitive to image aspect ratios, Human3R performs consistently well without camera intrinsics.

<span id="page-8-2"></span>

| Ablations            | WA-MPJPE↓ | W-MPJPE $\downarrow$ | $RTE\downarrow$ |
|----------------------|-----------|----------------------|-----------------|
| Human3R w/o Prior    | 221.2     | 808.4                | 2.2             |
| Human3R w/ ViT-S/672 | 129.9     | 314.2                | 2.2             |
| Human3R w/ ViT-B/672 | 122.1     | 292.9                | 2.2             |
| Human3R w/ ViT-L/672 | 113.6     | 291.7                | 2.2             |
| Human3R w/ ViT-L/896 | 112.2     | 267.9                | 2.2             |
| Naive w/o TTT3R      | 455.4     | 1263                 | 14.3            |
| Naive w/ TTT3R       | 401.3     | 1173.9               | 12.2            |
| Human3R w/o TTT3R    | 124.3     | 292.3                | 2.5             |
| Human3R w/ TTT3R     | 112.2     | 267.9                | 2.2             |

Table 3: Ablation of human prior and naive baselines in global human motion on EMDB-2 dataset, using different Multi-HMR ViT-DINO encoders and a simple combination of Multi-HMR and CUT3R as the naive baseline. Q Please check more detailed analyses in Sec. A.1, Sec. A.2, and Sec. A.3 of Sup. Mat.

#### 5 CONCLUSION

We presented Human3R, a one-stage method for 4D human-scene reconstruction, providing a feasible strategy for both efficient finetuning and real-time inference. Our method demonstrates competitive or state-of-the-art performance in both human motion recovery and general 3D reconstruction benchmark, and generalizes to casually captured videos.

Limitations & Future Work. Human3R represents an important first step towards feed-forward 3D human and scene reconstruction, but several limitations remain. First, our method relies on the head as the discriminative keypoint for detecting humans, which leads to failures when the head is not visible. Incorporating pixel-aligned body point localizers [40, 76] could mitigate this issue. Second, we currently represent humans using proxy SMPL meshes that do not model clothing or appearance. Extending the framework with 3DGS anchored on SMPL would enable richer, more holistic reconstructions. Third, while Human3R is designed as an online method for real-time applications, it can also serve as an effective initialization for optimization-based approaches [53] to improve accuracy at the cost of additional computation. Beyond these limitations, Human3R opens avenues for broader applications. Although our focus is on reconstructing humans from monocular videos, the underlying principles can extend to other dynamic entities. By leveraging spatial and temporal cues, the framework could be adapted to reconstruct animals, vehicles, or other moving objects with full 6D poses (see limitations Sec. D of Sup.Mat.). Such extensions would enable applications in wildlife monitoring, traffic analysis, human-object interaction, and robotics.

### 6 ACKNOWLEDGE

Thank all members of Endless AI, Inception3D and RVH Group for help, and Yiru for creating the fantastic logo — love it! Yue and Xingyu are funded by the Westlake Education Foundation. Gerard and Yuxuan are funded by the Carl Zeiss Foundation, the Deutsche Forschungsgemeinschaft - 409792180 (EmmyNoether Programme, project: Real Virtual Humans), and the German Federal Ministry of Education and Research: Tubingen AI Center, FKZ: 01IS18039A. Gerard is a member of ¨ the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645.

##