# <span id="page-0-1"></span>VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control

Sixiao Zheng<sup>1</sup>,<sup>2</sup> , Minghao Yin<sup>3</sup> , Wenbo Hu<sup>4</sup>† , Xiaoyu Li<sup>4</sup> , Ying Shan<sup>4</sup> , Yanwei Fu<sup>1</sup>,2† <sup>1</sup>Fudan University <sup>2</sup>Shanghai Innovation Institute <sup>3</sup>HKU <sup>4</sup>ARC Lab, Tencent PCG

Project Page: [https://sixiaozheng.github.io/VerseCrafter\\_page/](https://sixiaozheng.github.io/VerseCrafter_page/)

<span id="page-0-0"></span>![](_page_0_Figure_4.jpeg)

Figure 1. VerseCrafter enables precise control of camera motion and multi-object motion via a 4D Geometric Control representation built from a static background point cloud and per-object 3D Gaussian trajectories, producing videos that better follow the desired motion than Yume [\[61\]](#page-10-0) and Uni3C [\[11\]](#page-8-0) and closely match the ground-truth video.

# Abstract

*Video world models aim to simulate dynamic, real-world environments, yet existing methods struggle to provide unified and precise control over camera and multi-object motion, as videos inherently operate dynamics in the projected 2D image plane. To bridge this gap, we introduce VerseCrafter, a 4D-aware video world model that enables explicit and coherent control over both camera and object dynamics within a unified 4D geometric world state. Our approach is centered on a novel 4D Geometric Control representation, which encodes the world state through a static background point cloud and per-object 3D Gaussian* *trajectories. This representation captures not only an object's path but also its probabilistic 3D occupancy over time, offering a flexible, category-agnostic alternative to rigid bounding boxes or parametric models. These 4D controls are rendered into conditioning signals for a pretrained video diffusion model, enabling the generation of high-fidelity, view-consistent videos that precisely adhere to the specified dynamics. Unfortunately, another major challenge lies in the scarcity of large-scale training data with explicit 4D annotations. We address this by developing an automatic data engine that extracts the required 4D controls from in-the-wild videos, allowing us to train our model on a massive and diverse dataset.*

<sup>†</sup>Corresponding authors.

# <span id="page-1-0"></span>1. Introduction

Video world models learn to simulate environmental dynamics by generating future frame sequences conditioned on past observations and control signals, such as actions or camera trajectory [\[13,](#page-8-1) [29,](#page-9-0) [40,](#page-9-1) [46,](#page-9-2) [61\]](#page-10-0). They provide a unified interface for visual prediction [\[31\]](#page-9-3), navigation [\[7\]](#page-8-2), and manipulation [\[23\]](#page-9-4). However, the reliance on video introduces a fundamental challenge: while an ideal world model should simulate the full 4D spatiotemporal space to reflect our physical reality, videos inherently operate dynamics in the projected 2D image plane.

To bridge this gap, recent works incorporate camera control into video generation using explicit 3D geometry [\[11,](#page-8-0) [115,](#page-12-0) [125\]](#page-13-0), implicit pose embeddings [\[50\]](#page-10-1), or learned movement embeddings [\[9,](#page-8-3) [12,](#page-8-4) [68\]](#page-10-2). However, these methods are often limited to static scenes or leave the motion of multiple objects uncontrolled. To control object motion, existing approaches typically rely on 2D cues such as point trajectories [\[96\]](#page-11-0), optical flow [\[56\]](#page-10-3), masks [\[123\]](#page-13-1), or bounding boxes [\[89\]](#page-11-1), which lack 3D awareness and often fail under large viewpoint changes. More advanced 3D-aware methods use depth maps [\[122\]](#page-13-2), sparse 3D trajectories [\[15\]](#page-8-5), 3D bounding boxes [\[92\]](#page-11-2), or parametric human models like SMPL-X [\[11\]](#page-8-0) to align camera and object motion in 3D space. Nevertheless, these control spaces are inadequate for representing multi-object dynamics in a compact, flexible, and editable 4D state that is also naturally aligned with camera control. For instance, sparse trajectories are often noisy and incomplete, 3D bounding boxes impose rigid constraints ill-suited to natural objects, and SMPL-X representations are category-limited. Besides, several existing works focus on synthetic game environments [\[40,](#page-9-1) [109,](#page-12-1) [113\]](#page-12-2), where precise annotations are available for training, yet, the modeling of complex, realistic 4D worlds with multi-object dynamics remains underexplored.

Thus we propose *VerseCrafter*, a realistic, dynamic video world model that allows explicit and precise control over camera and multi-object motion in a unified 4D geometric control, as shown in Fig. [1.](#page-0-0) At its core is our novel *4D Geometric Control* representation, which encodes the world state using a static background point cloud for scene geometry and per-object 3D Gaussian trajectories to capture object dynamics. Each 3D Gaussian trajectory represents an object's probabilistic 3D occupancy over time: its mean defines the motion path, while its covariance captures the object's spatial extent and orientation. This probabilistic formulation provides a soft, flexible, and category-agnostic approach to modeling diverse object shapes and motions, overcoming the limitations of rigid 3D bounding boxes or category-specific parametric models. Crucially, the background point cloud and per-object 3D Gaussian trajectories share a common world coordinate system, enabling coherent and unified control over both camera and object motion. By rendering our 4D Geometric Control into target views, we condition a frozen Wan2.1-14B video diffusion backbone [\[86\]](#page-11-3) via a lightweight GeoAdapter, an adapter-style branch inspired by ControlNet [\[117\]](#page-12-3). This enables the generation of high-fidelity videos that accurately reflect the underlying 4D world state with specified camera and object dynamics. Unlike 2D control signals, our 4D Geometric Control is inherently 3D-aware, *i.e*. view-consistent and robust to occlusions, making it a more effective and reliable interface for video world modeling. Training VerseCrafter requires large-scale paired data of real-world videos and their corresponding 4D geometric controls. To this end, we constructed *VerseControl4D*, a large-scale real-world dataset with automatically annotated camera and object trajectories needed to construct our 4D geometric controls. This dataset allows us to train VerseCrafter on a massive and diverse set of real-world videos, significantly enhancing its generalization and performance.

Our contributions are threefold:

- We introduce a novel *4D Geometric Control* representation that unifies camera and multi-object motion in a shared 4D space. Its use of 3D Gaussian trajectories offers a flexible and category-agnostic way to control object dynamics, overcoming the limitations of rigid, categoryspecific models.
- We present *VerseCrafter*, a geometry-driven video world model that leverages our 4D Geometric Control to offer explicit and precise control over both camera and object motion. This enables the creation of high-fidelity, viewconsistent videos that accurately follow complex 4D instructions.
- We constructed an *VerseControl4D*, a large-scale realworld dataset with automatically annotated camera and object trajectories. This breakthrough solves a key data bottleneck, enabling us to train our model on a massive and diverse real-world dataset for superior generalization.

# 2. Related Works

Video World Models. World models learn environment dynamics from observations by predicting future states for downstream simulation, planning, and control [\[29,](#page-9-0) [30,](#page-9-5) [46\]](#page-9-2). Early visual world models adopt recurrent and latentvariable architectures [\[16,](#page-8-6) [24,](#page-9-6) [28,](#page-9-7) [62,](#page-10-4) [66,](#page-10-5) [85\]](#page-11-4), while recent approaches use large-scale transformer and diffusion backbones to roll out high-fidelity videos conditioned on actions, text, or camera trajectories [\[1,](#page-8-7) [2,](#page-8-8) [6,](#page-8-9) [9,](#page-8-3) [12,](#page-8-4) [20,](#page-8-10) [35,](#page-9-8) [39,](#page-9-9) [44,](#page-9-10) [48,](#page-10-6) [68,](#page-10-2) [86,](#page-11-3) [101,](#page-12-4) [108,](#page-12-5) [113\]](#page-12-2), and further extend temporal horizons with explicit memories or long-sequence models [\[50,](#page-10-1) [71,](#page-10-7) [99\]](#page-12-6). Geometry-aware works such as DeepVerse [\[13\]](#page-8-1), Voyager [\[40\]](#page-9-1), and Yume [\[61\]](#page-10-0) incorporate 3D structure to support 4D video generation and exploration, but are mainly controlled via text, actions, or camera tokens and do not expose a compact, editable 4D geometric state for <span id="page-2-1"></span>real multi-object dynamics. In contrast, VerseCrafter learns a geometry-driven mapping from 4D Geometric Control to dynamically realistic videos, enabling disentangled control over camera and multi-object motion.

3D World Generation. Recent work leverages powerful 2D generative priors to synthesize explorable 3D environments from text, images, or videos [\[47,](#page-9-11) [119\]](#page-12-7). Early methods mainly target object-level or single-scene generation [\[19,](#page-8-11) [37,](#page-9-12) [72,](#page-10-8) [116,](#page-12-8) [118,](#page-12-9) [120\]](#page-12-10), distilling image diffusion models [\[77\]](#page-11-5) into NeRFs [\[63\]](#page-10-9), implicit fields, meshes, or 3D Gaussian splats [\[43\]](#page-9-13), or optimizing per-scene geometry from multi-view or panoramic observations [\[18,](#page-8-12) [78,](#page-11-6) [111,](#page-12-11) [112,](#page-12-12) [114\]](#page-12-13). More recent approaches scale up to navigable 3D worlds [\[7\]](#page-8-2), combining depth estimation [\[106\]](#page-12-14), cameraguided video diffusion, iterative inpainting, and panoramic inputs to construct room- or city-scale Gaussian scenes for exploration [\[14,](#page-8-13) [52,](#page-10-10) [58,](#page-10-11) [59,](#page-10-12) [79,](#page-11-7) [84,](#page-11-8) [90,](#page-11-9) [109,](#page-12-1) [127\]](#page-13-3). However, these pipelines largely model static, synthetic-like geometry and offer limited explicit control over real multi-object dynamics, whereas VerseCrafter operates on real-world videos and uses a background point cloud plus per-object 3D Gaussian trajectories as an explicit 4D control state for worldconsistent dynamic video generation.

Controllable Video Generation. Controllable video generation aims to steer camera and object motion via explicit conditioning signals. Camera-controlled models [\[3,](#page-8-14) [4,](#page-8-15) [33,](#page-9-14) [45,](#page-9-15) [51,](#page-10-13) [82,](#page-11-10) [104,](#page-12-15) [124\]](#page-13-4) such as MotionCtrl [\[96\]](#page-11-0) and CameraCtrl [\[32\]](#page-9-16) inject camera extrinsics, Plucker-style encodings, ¨ or other 3D priors [\[11,](#page-8-0) [21,](#page-8-16) [27,](#page-9-17) [38,](#page-9-18) [73,](#page-10-14) [76,](#page-11-11) [97,](#page-12-16) [102,](#page-12-17) [115,](#page-12-0) [122,](#page-13-2) [125\]](#page-13-0) into video diffusion models to achieve precise viewpoint control, but mostly assume static or weakly dynamic scenes. Object motion [\[10,](#page-8-17) [25,](#page-9-19) [34,](#page-9-20) [49,](#page-10-15) [53,](#page-10-16) [55,](#page-10-17) [60,](#page-10-18) [64,](#page-10-19) [65,](#page-10-20) [67,](#page-10-21) [74,](#page-11-12) [80,](#page-11-13) [81,](#page-11-14) [83,](#page-11-15) [87,](#page-11-16) [88,](#page-11-17) [94,](#page-11-18) [95,](#page-11-19) [98,](#page-12-18) [105,](#page-12-19) [107,](#page-12-20) [110,](#page-12-21) [121,](#page-13-5) [126\]](#page-13-6) is typically controlled using 2D cues (bounding boxes, masks, trajectories, strokes, optical flow) as in Boximator [\[89\]](#page-11-1), DragAnything [\[100\]](#page-12-22), and MotionCanvas [\[103\]](#page-12-23), or with more 3D-aware signals such as depth maps, sparse 3D trajectories, 3D boxes, or SMPL-X bodies in I2V3D [\[122\]](#page-13-2), Uni3C [\[11\]](#page-8-0), CineMaster [\[92\]](#page-11-2), and Perception-as-Control [\[15\]](#page-8-5). While these methods substantially improve controllability, 2D controls remain view-dependent and fragile under large camera changes, and many 3D controls are categoryspecific, rigid, or tied to reconstruction-heavy pipelines. Recent approaches [\[15,](#page-8-5) [22,](#page-9-21) [26,](#page-9-22) [56,](#page-10-3) [92,](#page-11-2) [96,](#page-11-0) [103,](#page-12-23) [107,](#page-12-20) [125\]](#page-13-0) begin to jointly control camera and object motion, but their control spaces are still fragmented rather than a unified, compact world state. VerseCrafter instead introduces *4D Geometric Control*: a lightweight, category-agnostic world state where a background point cloud and per-object 3D Gaussian trajectories in a shared frame jointly drive camera and multi-object motion.

# 3. Method

We propose VerseCrafter, a geometry-driven video world model that maps an explicit 4D geometric world state into dynamic, realistic videos with disentangled control over camera and multi-object motion. Our design has two key components: (i) a unified *4D Geometric Control* (Sec. [3.1\)](#page-2-0) representation defined in a shared world coordinate frame, and (ii) a lightweight *GeoAdapter* (Sec. [3.2\)](#page-3-0) that injects rendered geometric signals into a frozen Wan2.1-14B backbone, so that edits to the 4D state directly reshape the generated video while preserving Wan2.1's strong visual prior. Given an input frame, a text prompt, and 4D Geometric Control, we model the world state as a static background point cloud and per-object 3D Gaussian trajectories, render them into multi-channel control maps, and feed these maps into GeoAdapter attached to Wan2.1.

### <span id="page-2-0"></span>3.1. 4D Geometric Control

We represent the state of the video world model as a 4D geometric world state, which we term *4D Geometric Control*. It is an explicit, editable state consisting of a static background point cloud P bg and per-object 3D Gaussian trajectories {G<sup>t</sup> <sup>o</sup>}, all defined in a shared world coordinate frame. Background point cloud. As in Fig. [2,](#page-3-1) we start from the input image, estimate monocular depth with MoGe-2 [\[93\]](#page-11-20), and obtain instance masks {Mo} with Grounded SAM2 [\[75\]](#page-11-21), where the user selects one or more objects to be controlled via text prompts or clicks. With camera intrinsics K and extrinsics (R1, t1), each pixel u = (u, v, 1)<sup>⊤</sup> with depth D1(u) is back-projected as

$$\mathbf{p}(\mathbf{u}) = \mathbf{R}_1^{\top} (D_1(\mathbf{u}) \mathbf{K}^{-1} \mathbf{u} - \mathbf{t}_1). \tag{1}$$

We use the instance masks to partition the reconstructed point cloud into per-object point clouds

$$P_o = \left\{ \mathbf{x}_{o,k} \,\middle|\, \mathbf{x}_{o,k} = \mathbf{p}(\mathbf{u}_k), \,\mathbf{u}_k \in M_o \right\},\tag{2}$$

and a static background cloud

$$P^{\text{bg}} = \left\{ \mathbf{p}(\mathbf{u}) \mid \mathbf{u} \notin \bigcup_{o} M_o \right\} = \left\{ \mathbf{p}_i \right\}_{i=1}^{N_{\text{bg}}}.$$
 (3)

During generation, the background at frame t is obtained by rendering P bg with the camera pose, so that viewpoint changes are realized as rigid camera motion in a fixed 3D world rather than by hallucinating a new background at every frame.

3D Gaussian trajectories. A single 3D Gaussian Go(x) = N (x | µ<sup>o</sup> , Σo) in the world frame compactly encodes an object's position (through µ<sup>o</sup> ), approximate shape and size (through the eigenvalues of Σo), and orientation (through its eigenvectors). A *3D Gaussian trajectory* for object o is then defined as a sequence of Gaussians

$$\{\mathcal{G}_o^t\}_{t=1}^T, \quad \mathcal{G}_o^t(\mathbf{x}) = \mathcal{N}(\mathbf{x} \mid \boldsymbol{\mu}_o^t, \boldsymbol{\Sigma}_o^t),$$
 (4)

<span id="page-3-2"></span><span id="page-3-1"></span>![](_page_3_Figure_0.jpeg)

Figure 2. Framework of VerseCrafter. Given an input image and a text prompt, we first estimate depth and obtain user-specified object masks to construct a 4D Geometric Control state consisting of a static background point cloud and per-object 3D Gaussian trajectories in a shared world frame. This state is rendered into background RGB/depth, 3D Gaussian trajectory RGB/depth, and a soft control mask for each frame, forming multi-channel 4D control maps. The control maps are encoded and fed into the proposed GeoAdapter, which conditions a frozen Wan2.1-14B video diffusion backbone together with text embeddings from umT5, enabling geometry-consistent video generation with precise control over camera and multi-object motion.

whose means  $\{\mu_o^t\}$  trace the motion path in 3D, while the covariances  $\{\Sigma_o^t\}$  capture how the object's spatial extent and orientation evolve over time. This probabilistic formulation describes the object's 3D occupancy in a soft, continuous manner and yields a compact control space that is more flexible than rigid 3D bounding boxes and more category-agnostic than parametric body models.

To initialize the trajectory for each controllable object o, we fit a full-covariance Gaussian to its point cloud  $P_o$  obtained in the previous step:

$$\boldsymbol{\mu}_o = \frac{1}{N_o} \sum_k \mathbf{x}_{o,k}, \boldsymbol{\Sigma}_o = \frac{1}{N_o} \sum_k (\mathbf{x}_{o,k} - \boldsymbol{\mu}_o) (\mathbf{x}_{o,k} - \boldsymbol{\mu}_o)^{\top},$$
(5)

which gives an initial Gaussian  $\mathcal{G}_o(\mathbf{x})$ 

The low-dimensional parameters  $\{\mu_o^t, \Sigma_o^t\}$  naturally support flexible, user-driven editing. In practice, we convert each  $\mathcal{G}_o^t$  into an ellipsoid mesh for visualization in a 3D editor such as Blender, and let the user specify or refine the trajectory by dragging and keyframing this ellipsoid in world space. The edited poses and shapes are mapped back to the  $\{\mu_o^t, \Sigma_o^t\}$  as control signals. The ellipsoids are only a user interface; all conditioning maps used by our model are rendered directly from the underlying 3D Gaussians.

**Rendering 4D control maps.** Given 4D Geometric Control, we render per-frame conditioning maps in the target camera views. For each frame t, we generate three types of maps: (i) background RGB/depth, RGB<sub>t</sub><sup>bg</sup> and Depth<sub>t</sub><sup>bg</sup>, by projecting the static cloud  $P^{bg}$  with the camera pose ( $\mathbf{R}_t, \mathbf{t}_t$ ); (ii) 3D Gaussian trajectory RGB/depth, RGB<sub>t</sub><sup>traj</sup> and Depth<sub>t</sub><sup>traj</sup>, by projecting the per-object Gaussians { $\mathcal{G}_o^t$ } into soft elliptical footprints and taking depth from the cor-

responding ellipsoid surfaces; (iii) a soft control mask  $M_t$  that indicates regions where the diffusion model should synthesize or overwrite content, obtained by inverting the valid background visibility and merging it with the projected 3D Gaussian footprints, followed by Gaussian smoothing. For the first frame t=1, we replace  $\mathrm{RGB}_1^\mathrm{bg}$  with input image and set  $M_1=0$ , so that the first frame is preserved and only future frames are modified. Background and 3D Gaussian maps share the same world state but are rendered through decoupled channels, so camera edits only affect background branch and object edits only affect 3D Gaussian trajectory branch, enabling geometry-consistent control.

#### <span id="page-3-0"></span>3.2. VerseCrafter Architecture

**Backbone.** We adopt Wan2.1-14B [86] as a frozen latent video diffusion / flow-matching backbone with a 3D VAE and a DiT-based denoiser. VerseCrafter treats Wan2.1 as a generic video prior: we do not change its architecture or weights, and instead attach a lightweight geometric adapter that conditions the backbone on our 4D control maps.

**GeoAdapter.** For each frame t, we take the rendered background and 3D Gaussian trajectory maps,  $RGB_t^{bg}$ ,  $Depth_t^{bg}$ ,  $RGB_t^{traj}$ ,  $Depth_t^{traj}$ , together with the soft control mask  $M_t$ . The four RGB/depth maps are encoded by the same 3D VAE as the video latent, while  $M_t$  is reshaped and interpolated to the latent resolution, following the practice in [41, 86]. Stacking along the temporal dimension yields a spatio–temporal geometry tensor, which is concatenated channel-wise and aligned with the latent video tokens. GeoAdapter is a lightweight DiT-style branch that operates on this geometry tensor. It shares the same token dimensions

<span id="page-4-1"></span><span id="page-4-0"></span>![](_page_4_Figure_0.jpeg)

Figure 3. Starting from Sekai-Real-HQ and SpatialVID-HQ, we obtain 81-frame clips extraction, followed by quality filtering. For each retained clip, Qwen2.5-VL-72B, Grounded-SAM2, and MegaSAM provide captions, object masks, depth, and camera poses, which are lifted into background/object point clouds, fitted with 3D Gaussian trajectories, and rendered as background/trajectory maps plus a merged mask that constitute our 4D Geometric Control.

sionality as the Wan-DiT blocks, but uses far fewer layers. We interleave GeoAdapter blocks with the frozen Wan-DiT: every k-th DiT block in Wan2.1 is paired with a GeoAdapter block whose output is linearly projected back to the backbone width and added as a residual modulation to the corresponding DiT block. Text prompts are encoded by umT5 [\[17\]](#page-8-18) into text embeddings, which are injected into both Wan's DiT blocks and GeoAdapter through the same text-conditioning interfaces. This adapter-based conditioning injects 4D geometric information into Wan2.1 with only a small number of extra parameters, while keeping all backbone weights fixed.

Inference. At inference time, VerseCrafter supports both independent control of camera or object motion and joint control of both within a single unified framework. For *camera-only* control, we provide a camera trajectory and background control maps while setting all trajectory-related channels (RGB/depth/mask) to zero. For *object-only* control, we keep the camera pose fixed, render a static background branch (RGB/depth and its mask) from P bg. For *joint* control, both branches are active and rendered from the same 4D world state, allowing VerseCrafter to adjust camera trajectory and multi-object motion in a coordinated, geometry-consistent manner.

# 4. VerseControl4D Dataset

To train and evaluate VerseCrafter on real, complex scenes with explicit 4D control, we construct VerseControl4D, a real-world video dataset with automatically derived 4D Geometric Control annotations. As shown in Fig. [3,](#page-4-0) VerseControl4D is built through four stages: data collection, clip extraction, quality filtering, and data annotation.

Data collection. VerseControl4D is built from two recent world-exploration datasets, Sekai-Real-HQ [\[54\]](#page-10-22) and SpatialVID-HQ [\[91\]](#page-11-22), which provide long in-the-wild videos with diverse outdoor and urban scenes, camera poses, and captions, but no object-motion labels. We take their high-resolution videos as the raw pool for constructing our 4D Geometric Control annotations.

Clip extraction. We apply PySceneDetect to detect shots in the videos. For each shot longer than 81 frames, we uniformly sample an 81-frame sub-clip and discard shorter shots, matching the default temporal length used by the Wan2.1 backbone.

Quality filtering. We apply an object-centric filtering pipeline to retain clips with clean geometry and controllable foreground. Using Grounded-SAM2 with prompts such as *"person . human . car . animal"*, we first obtain instance masks on the first frame and keep only clips whose controllable object count lies in [1, 6]. We then discard clips where any instance mask covers more than 20% of the image area. For human instances, we further remove clips whose masks touch image borders or whose aspect ratios fall outside [2, 4], as these typically correspond to severely truncated pedestrians. Finally, we apply visual-quality filtering (aesthetic and luminance scores) to exclude blurry or over-/under-exposed clips, yielding a set of visually clean, structurally reliable videos.

Data annotation. We then annotate each filtered clip with 4D Geometric Control. We first generate a descriptive caption using Qwen2.5-VL-72B [\[5\]](#page-8-19), which serves as the text prompt during training. For geometry, we adopt MegaSAM as the base pipeline and replace its monocular and metric depth modules with MoGe-2 [\[93\]](#page-11-20) and UniDepth V2 [\[70\]](#page-10-23), respectively, to obtain more accurate and temporally consistent depth. Given the video frames, the depth, and the camera trajectory, we reconstruct a 3D point cloud for every frame. Applying Grounded-SAM2 instance masks on each frame to these point clouds yields per-object point clouds and a static background point cloud P bg, as described in Sec. [3.1.](#page-2-0) For each object, we then fit per-frame 3D Gaussians and connect them into a 3D Gaussian trajectory {G<sup>t</sup> <sup>o</sup>}. Finally, we render the 4D Geometric Control into modelready signals. The background point cloud is rendered with the camera trajectory to obtain background RGB, depth and mask. The 3D Gaussian trajectories are rendered into trajectory RGB, depth and mask. We invert the background mask and merge it with the trajectory mask to produce a final merged mask that marks regions where the video diffusion model should synthesize content.

In total, VerseControl4D contains 35,000 training samples and 1,000 validation samples. In the training set, about

<span id="page-5-2"></span><span id="page-5-0"></span>![](_page_5_Figure_0.jpeg)

Figure 4. Qualitative comparison of joint camera and object motion control. Perception-as-Control often yields low-fidelity frames with inaccurate camera motion, Yume roughly follows the text-described motion but lacks precise control, and Uni3C is limited to human motion. VerseCrafter more faithfully follows both the camera trajectory and multi-object motion while maintaining sharp appearance and geometrically consistent backgrounds.

<span id="page-5-1"></span>![](_page_5_Figure_2.jpeg)

Figure 5. Qualitative comparison of camera-only motion control on static scenes. ViewCrafter, Voyager, and FlashWorld often exhibit distorted facades, drifting structures, or inconsistent parallax along the camera path. VerseCrafter better follows the target trajectory while preserving sharp details and globally consistent 3D geometry.

26% of samples are sourced from Sekai-Real-HQ and 74% from SpatialVID-HQ, and 20% of the samples depict static scenes, encouraging VerseCrafter to learn both camera-only world exploration and coupled camera–object dynamics. The validation set additionally includes 250 static-scene samples to specifically assess camera-only control.

# 5. Experiments

Implementation Details. We build VerseCrafter on top of the Wan2.1 T2V-14B model. The Wan backbone is kept frozen and only the GeoAdapter is updated. Each GeoAdapter block is initialized from the weights of its paired DiT block in Wan2.1 to stabilize training, and we set k = 5 so that every 5-th DiT block in Wan2.1 is paired with a GeoAdapter block. We use the Adam optimizer with a learning rate of 2e − 5, 100 warmup steps, and a constant-with-warmup learning-rate schedule. All experiments are conducted on 16 96GB GPUs with a global batch size of 16. Training is performed in two stages: we first train for 2,500 iterations on 480P clips, and then fine-tune the same model for another 2,500 iterations on 720P clips. The total wallclock training time is about 380 hours. We adopt classifierfree guidance during training by randomly dropping the text condition with probability 0.1. At inference time, we use 50 denoising steps and a classifier-free guidance scale of 5.0. Generating an 81-frame 720P video clip on 8 96GB GPU takes about 1152 seconds, with a peak memory usage of about 90 GB.

Evaluation Metrics. We evaluate overall video quality using VBench-I2V. For camera control, we follow prior camera-control work [\[32\]](#page-9-16) and report rotation error (RotErr) and translation error (TransErr). For object-motion control, we adopt ObjMC proposed in MotionCtrl [\[96\]](#page-11-0). Given a generated video, we run the same geometry annotation pipeline as in VerseControl4D to estimate its camera trajectory and 3D Gaussian trajectories, and compare them with the corresponding ground-truth trajectories from our dataset. ObjMC is computed as the average Euclidean distance between the estimated and ground-truth 3D Gaussian means over all controlled objects and frames.

<span id="page-6-4"></span><span id="page-6-0"></span>Table 1. **Joint camera and object motion control on VerseControl4D.** We report VBench-I2V scores and 3D control metrics (RotErr, TransErr, ObjMC;). VerseCrafter achieves the best overall video quality and the most accurate joint control of camera and object motion.

|                            | Overall<br>Score | Imaging<br>Quality | Aesthetic<br>Quality | Dynamic<br>Degree | Motion<br>Smoothness | Background<br>Consistency | Subject<br>Consistency | I2V<br>Background | I2V<br>Subject <sup>↑</sup> | RotErr↓ | TransErr↓ | ObjMC↓ |
|----------------------------|------------------|--------------------|----------------------|-------------------|----------------------|---------------------------|------------------------|-------------------|-----------------------------|---------|-----------|--------|
| Perception-as-Control [15] | 83.66            | 66.81              | 53.34                | 73.91             | 96.89                | 93.19                     | 94.02                  | 96.35             | 94.78                       | 5.006   | 8.767     | 6.556  |
| Yume [61]                  | 85.47            | 71.16              | 52.39                | 72.24             | 98.96                | 95.66                     | 96.43                  | 98.51             | 98.39                       | 7.560   | 8.735     | 7.959  |
| Uni3C [11]                 | 83.55            | 68.06              | 53.16                | 66.09             | 98.94                | 93.74                     | 94.19                  | 97.19             | 97.05                       | 1.361   | 7.731     | 5.883  |
| Ours                       | 88.10            | 72.70              | 57.49                | 86.26             | 98.79                | 95.69                     | 96.48                  | 98.76             | 98.65                       | 0.890   | 3.103     | 2.507  |

<span id="page-6-1"></span>Table 2. Camera-only motion control on static scenes. On the static subset of VerseControl4D, we report VBench-I2V scores and camera control metrics RotErr / TransErr. VerseCrafter achieves the best overall visual quality while substantially reducing camera pose errors.

|                   | Overall Score | Imaging<br>Quality | Aesthetic<br>Quality | Dynamic<br>Degree | Motion<br>Smoothness | Background<br>Consistency | Subject<br>Consistency | I2V<br>Background | I2V<br>Subject <sup>↑</sup> | RotErr↓ | TransErr↓ |
|-------------------|---------------|--------------------|----------------------|-------------------|----------------------|---------------------------|------------------------|-------------------|-----------------------------|---------|-----------|
| ViewCrafter [115] | 84.04         | 69.56              | 55.52                | 68.02             | 97.86                | 92.09                     | 94.25                  | 97.70             | 97.29                       | 2.101   | 9.868     |
| Voyager [40]      | 78.12         | 55.48              | 49.80                | 65.34             | 99.39                | 92.31                     | 91.55                  | 86.02             | 85.03                       | 3.557   | 3.880     |
| FlashWorld [52]   | 81.80         | 68.94              | 53.72                | 58.26             | 98.81                | 91.88                     | 94.44                  | 94.40             | 93.93                       | 2.748   | 10.010    |
| Ours              | 86.80         | 74.57              | 54.78                | 80.34             | 97.62                | 94.88                     | 95.55                  | 97.86             | 98.79                       | 0.650   | 2.587     |

<span id="page-6-2"></span>![](_page_6_Figure_4.jpeg)

Figure 6. **Ablation on object-motion representations.** We compare controlling objects with *3D point trajectory* (top), *3D bounding boxe* (middle), and *3D Gaussian trajectory* (fourth). 3D point trajectory and 3D bounding boxe often cause scale drift and misaligned motion (red boxes), whereas 3D Gaussian trajectory track the intended camera trajectory and preserve plausible shapes and background interactions.

### 5.1. Joint Camera and Object Motion Control

We first evaluate joint control of camera and object motion on VerseControl4D. As shown in Table 1, VerseCrafter achieves the best VBench-I2V scores among Perception-as-Control, Yume, Uni3C, and our model, with clear gains in Overall Score, Imaging Quality, Aesthetic Quality, and both Subject/Background consistency. On 3D control metrics, VerseCrafter substantially reduces rotation, translation, and object-motion errors compared with the strongest baseline, reflecting much tighter alignment with the target 4D trajectories. Qualitative comparisons in Fig. 4 further highlight these differences: Perception-as-Control often produces low-quality frames with inaccurate camera motion; Yume, driven only by text descriptions of motion, roughly follows the desired direction but lacks precise trajectory control; and Uni3C, relying on SMPL-X, can control human

<span id="page-6-3"></span>![](_page_6_Figure_8.jpeg)

Figure 7. **Ablation on depth-aware control.** We compare VerseCrafter without depth inputs (*Ours (w/o depth)*, top) and with RGB+depth control (middle) under the same camera trajectorym. Without depth, the model often misorders foreground and background, e.g., lampposts are pulled in front of distant buildings—and occlusion boundaries drift over time (red boxes). Adding depth restores consistent parallax and occlusion, producing geometry much closer to the ground truth.

motion but fails to handle other categories such as vehicles. In contrast, VerseCrafter keeps multiple objects attached to their 3D Gaussian trajectories while accurately following the specified camera path, yielding sharp and temporally coherent videos.

#### **5.2. Camera-Only Motion Control**

We evaluate camera-only control on the static-scene subset of VerseControl4D, where objects remain stationary and only the camera moves. As shown in Table 2, VerseCrafter achieves the best VBench-I2V performance among ViewCrafter, Voyager, FlashWorld, and our model, with consistent gains in Overall Score, Imaging Quality, and both background and subject consistency, while maintaining motion smoothness comparable to prior methods. On 3D camera metrics, VerseCrafter substantially reduces rotation and translation errors relative to the strongest baseline, indicating that it follows the target camera trajectory much more faithfully in static scenes. Qualitative comparisons in Fig. 5 further confirm these trends: baselines often exhibit bending walls, misaligned windows, or unstable paral-

<span id="page-7-0"></span>Table 3. Ablation study on 3D representation, depth, and decoupled controls. We compare different variants of VerseCrafter using VBench-I2V and 3D control metrics (RotErr, TransErr, ObjMC;). Our full model with 3D Gaussian trajectories, depth-aware rendering, and decoupled background/foreground controls achieves the best visual quality and the most accurate camera and object motion control.

|                            | Overall<br>Score ↑ | Imaging<br>Quality ↑ | Aesthetic<br>Quality ↑ | Dynamic<br>Degree ↑ | Motion<br>Smoothness↑ | Background<br>Consistency↑ | Subject<br>Consistency↑ | I2V<br>Background↑ | I2V   |       | Subject↑ RotErr↓ TransErr↓ ObjMC↓ |       |
|----------------------------|--------------------|----------------------|------------------------|---------------------|-----------------------|----------------------------|-------------------------|--------------------|-------|-------|-----------------------------------|-------|
| Ours (3D Bounding Box)     | 85.45              | 69.23                | 55.70                  | 78.57               | 98.70                 | 92.92                      | 93.27                   | 97.74              | 97.48 | 1.350 | 3.805                             | 4.520 |
| Ours (3D Point Trajectory) | 85.57              | 70.29                | 55.27                  | 78.23               | 98.63                 | 94.00                      | 92.75                   | 97.85              | 97.55 | 1.298 | 3.281                             | 6.896 |
| Ours (w/o depth)           | 85.64              | 70.19                | 55.00                  | 80.60               | 98.66                 | 92.07                      | 92.83                   | 98.07              | 97.69 | 1.177 | 3.900                             | 4.929 |
| Ours (BG & FG Merged)      | 85.72              | 69.19                | 54.86                  | 83.72               | 98.65                 | 91.15                      | 92.86                   | 97.93              | 97.41 | 1.080 | 3.803                             | 3.726 |
| Ours                       | 88.10              | 72.70                | 57.49                  | 86.26               | 98.79                 | 95.69                      | 96.48                   | 98.76              | 98.65 | 0.890 | 3.103                             | 2.507 |

<span id="page-7-1"></span>![](_page_7_Figure_2.jpeg)

Figure 8. Ablation on decoupled background / foreground controls. We compare merging background and foreground controls into a single map (*Ours (BG & FG Controls Merged)*, top) with our default decoupled design (middle). When controls are merged, object motion control performance significantly degrades (red box), while the separation design preserves the static background and produces sharper, more stable object motion.

lax along the path, whereas VerseCrafter preserves straight structures, stable depth relationships, and an appearance closer to the ground-truth video, evidencing precise camera control in a static 3D world.

### 5.3. Ablation Study

We conduct ablations to analyze three key design choices in VerseCrafter: (i) the object 3D representation in the control space, (ii) the use of depth in control maps, and (iii) the decoupling of background and foreground controls. All variants share the same training data, backbone, and optimization settings; only the control representation is changed.

3D representation of object motion. To isolate the effect of our motion representation, we derive two ablations from each per-frame 3D Gaussian: (1) an oriented 3D bounding box whose axes follow the Gaussian's principal directions and whose side lengths scale with its principal spreads; and (2) a 3D point trajectory that retains only the Gaussian centroid. The rest of the pipeline is unchanged—we simply rasterize cuboids (for boxes) or tiny disks/spheres (for points) instead of Gaussian ellipses. As reported in Table [3,](#page-7-0) replacing Gaussians with boxes slightly hurts both visual quality and control accuracy (Overall Score ↓ from 88.10 to 85.45; ObjMC ↑ from 2.51 to 4.52), while point trajectories give the weakest object-motion consistency (ObjMC = 6.90). Qualitatively (Fig. [6\)](#page-6-2), points and boxes often yield scale artifacts and misaligned motion, whereas 3D Gaussian trajectories better track the intended paths and preserve plausible object shapes.

Effect of depth. To evaluate the effect of depth, we removed the depth channel from the background and trajectory controls ("Ours (w/o depth)" in Table [3\)](#page-7-0). This variation resulted in a lower overall score and significantly worse 3D control (higher RotErr and ObjMC values). As shown in Figure [7,](#page-6-3) without depth, the model frequently misorders foreground and background: vertical structures like streetlights appear next to shelves in the foreground, while buildings that should be behind the character are positioned elsewhere, and occlusion boundaries drift over time. With RGB+depth control, With RGB+depth control, VerseCrafter recovers more consistent parallax and occlusion, producing geometry much closer to the ground truth.

Decoupled vs. merged controls. We further compare our decoupled design with a variant that merges background and 3D Gaussian trajectory maps into a single control stream (*Ours (BG & FG Merged)* in Table [3\)](#page-7-0). Although this variant still benefits from the explicit 4D state, it consistently underperforms the full model on VBench, with a particularly noticeable drop in object-motion accuracy (ObjMC increases from 2.51 to 3.73). As shown in Fig. [8,](#page-7-1) the merged control leads to a clear degradation in motion control for moving people. In contrast, keeping decoupled design preserves static geometry while producing more precise and stable object motion, which is crucial for accurate and geometry-consistent control.

# 6. Conclusion

We presented VerseCrafter, a geometry-driven video world model that exposes an explicit 4D Geometric Control state, built from a static background point cloud and perobject 3D Gaussian trajectories in a shared world frame. Coupled with the GeoAdapter that conditions a frozen Wan2.1 backbone, this design enables high-fidelity video generation with precise, disentangled control over camera and multi-object motion. To support training and evaluation, we constructed VerseControl4D, a large-scale realworld dataset with automatically annotated camera and object trajectories. Experiments and ablations show that VerseCrafter delivers superior visual quality and more accurate 3D control than existing controllable video generators and world models, highlighting 4D Geometric Control as a promising interface for future work on dynamic world simulation and editing.